{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML/DL Basics Exercisesipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOShCw9qI8rGI4L3yb7jM78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mindyng/ML-Keeping-it-FRESH/blob/master/ML_DL_Basics_Exercisesipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfdwo7-J1N5r"
      },
      "source": [
        "## Manual Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aki7lwRSX_km",
        "outputId": "41d520a0-5d53-49d7-f3e4-0353253128d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# List of weights/Mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    # Print the weights and initialize the loss\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    print(\"MSE=\", l_sum / 3)\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / 3)\n",
        "\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1MzKUwy1eYn"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Auto Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ2jAmLPYvW7",
        "outputId": "5eb23bd3-92be-4a36-fea4-d0cee2aaccbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2\n",
        "\n",
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4).item())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val) # 1) Forward pass\n",
        "        l = loss(y_pred, y_val) # 2) Compute loss\n",
        "        l.backward() # 3) Back propagation to update weights\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "# After training\n",
        "print(\"Prediction (after training)\",  4, forward(4).item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction (after training) 4 7.804864406585693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l8fyKkoZymg"
      },
      "source": [
        "## Pytorch-built Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-B4oIFA2yAH",
        "outputId": "677c0896-2d7f-4c3c-b0d5-241ab8072815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 25.841419219970703 \n",
            "Epoch: 1 | Loss: 11.512296676635742 \n",
            "Epoch: 2 | Loss: 5.13325834274292 \n",
            "Epoch: 3 | Loss: 2.293372631072998 \n",
            "Epoch: 4 | Loss: 1.0290179252624512 \n",
            "Epoch: 5 | Loss: 0.46604785323143005 \n",
            "Epoch: 6 | Loss: 0.2153146117925644 \n",
            "Epoch: 7 | Loss: 0.10358257591724396 \n",
            "Epoch: 8 | Loss: 0.053731758147478104 \n",
            "Epoch: 9 | Loss: 0.03142990916967392 \n",
            "Epoch: 10 | Loss: 0.021393848583102226 \n",
            "Epoch: 11 | Loss: 0.016819769516587257 \n",
            "Epoch: 12 | Loss: 0.014678658917546272 \n",
            "Epoch: 13 | Loss: 0.013622180558741093 \n",
            "Epoch: 14 | Loss: 0.013049978762865067 \n",
            "Epoch: 15 | Loss: 0.012694859877228737 \n",
            "Epoch: 16 | Loss: 0.012437851168215275 \n",
            "Epoch: 17 | Loss: 0.012225865386426449 \n",
            "Epoch: 18 | Loss: 0.012035384774208069 \n",
            "Epoch: 19 | Loss: 0.01185579877346754 \n",
            "Epoch: 20 | Loss: 0.011682509444653988 \n",
            "Epoch: 21 | Loss: 0.01151329930871725 \n",
            "Epoch: 22 | Loss: 0.011347251012921333 \n",
            "Epoch: 23 | Loss: 0.011183956637978554 \n",
            "Epoch: 24 | Loss: 0.011023059487342834 \n",
            "Epoch: 25 | Loss: 0.010864629410207272 \n",
            "Epoch: 26 | Loss: 0.010708415880799294 \n",
            "Epoch: 27 | Loss: 0.010554495267570019 \n",
            "Epoch: 28 | Loss: 0.010402850806713104 \n",
            "Epoch: 29 | Loss: 0.010253360494971275 \n",
            "Epoch: 30 | Loss: 0.010106019675731659 \n",
            "Epoch: 31 | Loss: 0.009960774332284927 \n",
            "Epoch: 32 | Loss: 0.009817583486437798 \n",
            "Epoch: 33 | Loss: 0.009676513262093067 \n",
            "Epoch: 34 | Loss: 0.009537412784993649 \n",
            "Epoch: 35 | Loss: 0.009400369599461555 \n",
            "Epoch: 36 | Loss: 0.009265268221497536 \n",
            "Epoch: 37 | Loss: 0.009132103063166142 \n",
            "Epoch: 38 | Loss: 0.009000842459499836 \n",
            "Epoch: 39 | Loss: 0.008871525526046753 \n",
            "Epoch: 40 | Loss: 0.00874399021267891 \n",
            "Epoch: 41 | Loss: 0.008618341758847237 \n",
            "Epoch: 42 | Loss: 0.00849449634552002 \n",
            "Epoch: 43 | Loss: 0.008372411131858826 \n",
            "Epoch: 44 | Loss: 0.008252106606960297 \n",
            "Epoch: 45 | Loss: 0.00813351385295391 \n",
            "Epoch: 46 | Loss: 0.008016594685614109 \n",
            "Epoch: 47 | Loss: 0.007901374250650406 \n",
            "Epoch: 48 | Loss: 0.007787827402353287 \n",
            "Epoch: 49 | Loss: 0.0076758842915296555 \n",
            "Epoch: 50 | Loss: 0.007565600797533989 \n",
            "Epoch: 51 | Loss: 0.007456868886947632 \n",
            "Epoch: 52 | Loss: 0.007349706254899502 \n",
            "Epoch: 53 | Loss: 0.007244037464261055 \n",
            "Epoch: 54 | Loss: 0.007140002679079771 \n",
            "Epoch: 55 | Loss: 0.007037351839244366 \n",
            "Epoch: 56 | Loss: 0.006936248857527971 \n",
            "Epoch: 57 | Loss: 0.006836533080786467 \n",
            "Epoch: 58 | Loss: 0.006738248281180859 \n",
            "Epoch: 59 | Loss: 0.006641423795372248 \n",
            "Epoch: 60 | Loss: 0.0065459939651191235 \n",
            "Epoch: 61 | Loss: 0.0064518870785832405 \n",
            "Epoch: 62 | Loss: 0.006359162274748087 \n",
            "Epoch: 63 | Loss: 0.0062678102403879166 \n",
            "Epoch: 64 | Loss: 0.006177715957164764 \n",
            "Epoch: 65 | Loss: 0.006088935770094395 \n",
            "Epoch: 66 | Loss: 0.006001422181725502 \n",
            "Epoch: 67 | Loss: 0.005915192887187004 \n",
            "Epoch: 68 | Loss: 0.005830186884850264 \n",
            "Epoch: 69 | Loss: 0.005746375769376755 \n",
            "Epoch: 70 | Loss: 0.005663829855620861 \n",
            "Epoch: 71 | Loss: 0.005582405254244804 \n",
            "Epoch: 72 | Loss: 0.005502146668732166 \n",
            "Epoch: 73 | Loss: 0.005423065274953842 \n",
            "Epoch: 74 | Loss: 0.005345171317458153 \n",
            "Epoch: 75 | Loss: 0.005268352571874857 \n",
            "Epoch: 76 | Loss: 0.005192632786929607 \n",
            "Epoch: 77 | Loss: 0.005117993801832199 \n",
            "Epoch: 78 | Loss: 0.005044472403824329 \n",
            "Epoch: 79 | Loss: 0.004971951246261597 \n",
            "Epoch: 80 | Loss: 0.004900504369288683 \n",
            "Epoch: 81 | Loss: 0.004830085672438145 \n",
            "Epoch: 82 | Loss: 0.004760655574500561 \n",
            "Epoch: 83 | Loss: 0.004692266695201397 \n",
            "Epoch: 84 | Loss: 0.004624806810170412 \n",
            "Epoch: 85 | Loss: 0.004558349959552288 \n",
            "Epoch: 86 | Loss: 0.0044928123243153095 \n",
            "Epoch: 87 | Loss: 0.0044282409362494946 \n",
            "Epoch: 88 | Loss: 0.004364633932709694 \n",
            "Epoch: 89 | Loss: 0.004301910754293203 \n",
            "Epoch: 90 | Loss: 0.00424007885158062 \n",
            "Epoch: 91 | Loss: 0.004179127048701048 \n",
            "Epoch: 92 | Loss: 0.004119069315493107 \n",
            "Epoch: 93 | Loss: 0.004059883765876293 \n",
            "Epoch: 94 | Loss: 0.004001551307737827 \n",
            "Epoch: 95 | Loss: 0.003944022580981255 \n",
            "Epoch: 96 | Loss: 0.0038873341400176287 \n",
            "Epoch: 97 | Loss: 0.003831432666629553 \n",
            "Epoch: 98 | Loss: 0.0037763877771794796 \n",
            "Epoch: 99 | Loss: 0.003722127992659807 \n",
            "Epoch: 100 | Loss: 0.003668640973046422 \n",
            "Epoch: 101 | Loss: 0.0036158873699605465 \n",
            "Epoch: 102 | Loss: 0.0035639600828289986 \n",
            "Epoch: 103 | Loss: 0.003512746188789606 \n",
            "Epoch: 104 | Loss: 0.003462224267423153 \n",
            "Epoch: 105 | Loss: 0.0034124981611967087 \n",
            "Epoch: 106 | Loss: 0.0033634451683610678 \n",
            "Epoch: 107 | Loss: 0.0033151106908917427 \n",
            "Epoch: 108 | Loss: 0.003267488442361355 \n",
            "Epoch: 109 | Loss: 0.0032205369789153337 \n",
            "Epoch: 110 | Loss: 0.0031742220744490623 \n",
            "Epoch: 111 | Loss: 0.0031285970471799374 \n",
            "Epoch: 112 | Loss: 0.003083650954067707 \n",
            "Epoch: 113 | Loss: 0.003039306029677391 \n",
            "Epoch: 114 | Loss: 0.002995652612298727 \n",
            "Epoch: 115 | Loss: 0.002952587092295289 \n",
            "Epoch: 116 | Loss: 0.002910182811319828 \n",
            "Epoch: 117 | Loss: 0.002868358977138996 \n",
            "Epoch: 118 | Loss: 0.002827121177688241 \n",
            "Epoch: 119 | Loss: 0.0027865057345479727 \n",
            "Epoch: 120 | Loss: 0.002746444894000888 \n",
            "Epoch: 121 | Loss: 0.002706956584006548 \n",
            "Epoch: 122 | Loss: 0.0026680787559598684 \n",
            "Epoch: 123 | Loss: 0.0026297385338693857 \n",
            "Epoch: 124 | Loss: 0.002591937780380249 \n",
            "Epoch: 125 | Loss: 0.0025546811521053314 \n",
            "Epoch: 126 | Loss: 0.002517971210181713 \n",
            "Epoch: 127 | Loss: 0.0024817832745611668 \n",
            "Epoch: 128 | Loss: 0.002446107566356659 \n",
            "Epoch: 129 | Loss: 0.002410934306681156 \n",
            "Epoch: 130 | Loss: 0.0023762849159538746 \n",
            "Epoch: 131 | Loss: 0.002342149382457137 \n",
            "Epoch: 132 | Loss: 0.0023084941785782576 \n",
            "Epoch: 133 | Loss: 0.002275322563946247 \n",
            "Epoch: 134 | Loss: 0.0022426042705774307 \n",
            "Epoch: 135 | Loss: 0.002210381207987666 \n",
            "Epoch: 136 | Loss: 0.002178613794967532 \n",
            "Epoch: 137 | Loss: 0.0021473108790814877 \n",
            "Epoch: 138 | Loss: 0.0021164698991924524 \n",
            "Epoch: 139 | Loss: 0.002086025895550847 \n",
            "Epoch: 140 | Loss: 0.0020560473203659058 \n",
            "Epoch: 141 | Loss: 0.0020265139173716307 \n",
            "Epoch: 142 | Loss: 0.00199737586081028 \n",
            "Epoch: 143 | Loss: 0.0019686680752784014 \n",
            "Epoch: 144 | Loss: 0.0019403770565986633 \n",
            "Epoch: 145 | Loss: 0.0019124990794807673 \n",
            "Epoch: 146 | Loss: 0.001884995843283832 \n",
            "Epoch: 147 | Loss: 0.001857912284322083 \n",
            "Epoch: 148 | Loss: 0.0018312076572328806 \n",
            "Epoch: 149 | Loss: 0.0018048925558105111 \n",
            "Epoch: 150 | Loss: 0.001778960577212274 \n",
            "Epoch: 151 | Loss: 0.0017534024082124233 \n",
            "Epoch: 152 | Loss: 0.0017282019834965467 \n",
            "Epoch: 153 | Loss: 0.0017033532494679093 \n",
            "Epoch: 154 | Loss: 0.0016788928769528866 \n",
            "Epoch: 155 | Loss: 0.0016547406557947397 \n",
            "Epoch: 156 | Loss: 0.0016309770289808512 \n",
            "Epoch: 157 | Loss: 0.0016075300518423319 \n",
            "Epoch: 158 | Loss: 0.0015844331355765462 \n",
            "Epoch: 159 | Loss: 0.0015616723103448749 \n",
            "Epoch: 160 | Loss: 0.0015392097411677241 \n",
            "Epoch: 161 | Loss: 0.001517096534371376 \n",
            "Epoch: 162 | Loss: 0.0014953108038753271 \n",
            "Epoch: 163 | Loss: 0.0014738133177161217 \n",
            "Epoch: 164 | Loss: 0.0014526156010106206 \n",
            "Epoch: 165 | Loss: 0.0014317571185529232 \n",
            "Epoch: 166 | Loss: 0.0014111697673797607 \n",
            "Epoch: 167 | Loss: 0.0013908692635595798 \n",
            "Epoch: 168 | Loss: 0.0013708891347050667 \n",
            "Epoch: 169 | Loss: 0.001351198647171259 \n",
            "Epoch: 170 | Loss: 0.0013317686971276999 \n",
            "Epoch: 171 | Loss: 0.001312642591074109 \n",
            "Epoch: 172 | Loss: 0.001293763518333435 \n",
            "Epoch: 173 | Loss: 0.001275177812203765 \n",
            "Epoch: 174 | Loss: 0.001256840187124908 \n",
            "Epoch: 175 | Loss: 0.0012387847527861595 \n",
            "Epoch: 176 | Loss: 0.0012209948617964983 \n",
            "Epoch: 177 | Loss: 0.0012034408282488585 \n",
            "Epoch: 178 | Loss: 0.0011861308012157679 \n",
            "Epoch: 179 | Loss: 0.0011690864339470863 \n",
            "Epoch: 180 | Loss: 0.0011522966669872403 \n",
            "Epoch: 181 | Loss: 0.0011357174953445792 \n",
            "Epoch: 182 | Loss: 0.0011194043327122927 \n",
            "Epoch: 183 | Loss: 0.0011033376213163137 \n",
            "Epoch: 184 | Loss: 0.0010874455329030752 \n",
            "Epoch: 185 | Loss: 0.0010718372650444508 \n",
            "Epoch: 186 | Loss: 0.0010564362164586782 \n",
            "Epoch: 187 | Loss: 0.00104123679921031 \n",
            "Epoch: 188 | Loss: 0.0010262945434078574 \n",
            "Epoch: 189 | Loss: 0.0010115231852978468 \n",
            "Epoch: 190 | Loss: 0.0009970057290047407 \n",
            "Epoch: 191 | Loss: 0.0009826780296862125 \n",
            "Epoch: 192 | Loss: 0.0009685445111244917 \n",
            "Epoch: 193 | Loss: 0.0009546256624162197 \n",
            "Epoch: 194 | Loss: 0.0009409082122147083 \n",
            "Epoch: 195 | Loss: 0.0009273893665522337 \n",
            "Epoch: 196 | Loss: 0.0009140552720054984 \n",
            "Epoch: 197 | Loss: 0.0009009127970784903 \n",
            "Epoch: 198 | Loss: 0.0008879712549969554 \n",
            "Epoch: 199 | Loss: 0.0008752109715715051 \n",
            "Epoch: 200 | Loss: 0.0008626336930319667 \n",
            "Epoch: 201 | Loss: 0.0008502346463501453 \n",
            "Epoch: 202 | Loss: 0.000838021282106638 \n",
            "Epoch: 203 | Loss: 0.0008259824826382101 \n",
            "Epoch: 204 | Loss: 0.0008141069556586444 \n",
            "Epoch: 205 | Loss: 0.0008024047710932791 \n",
            "Epoch: 206 | Loss: 0.0007908657426014543 \n",
            "Epoch: 207 | Loss: 0.0007795239216648042 \n",
            "Epoch: 208 | Loss: 0.0007683028234168887 \n",
            "Epoch: 209 | Loss: 0.000757262168917805 \n",
            "Epoch: 210 | Loss: 0.0007463847869075835 \n",
            "Epoch: 211 | Loss: 0.0007356492569670081 \n",
            "Epoch: 212 | Loss: 0.0007250861381180584 \n",
            "Epoch: 213 | Loss: 0.0007146709831431508 \n",
            "Epoch: 214 | Loss: 0.0007043974474072456 \n",
            "Epoch: 215 | Loss: 0.0006942665204405785 \n",
            "Epoch: 216 | Loss: 0.0006842957227490842 \n",
            "Epoch: 217 | Loss: 0.0006744550773873925 \n",
            "Epoch: 218 | Loss: 0.0006647569825872779 \n",
            "Epoch: 219 | Loss: 0.0006552072591148317 \n",
            "Epoch: 220 | Loss: 0.0006457938579842448 \n",
            "Epoch: 221 | Loss: 0.0006365115405060351 \n",
            "Epoch: 222 | Loss: 0.0006273657199926674 \n",
            "Epoch: 223 | Loss: 0.0006183431250974536 \n",
            "Epoch: 224 | Loss: 0.0006094677373766899 \n",
            "Epoch: 225 | Loss: 0.0006007135380059481 \n",
            "Epoch: 226 | Loss: 0.0005920756375417113 \n",
            "Epoch: 227 | Loss: 0.0005835649790242314 \n",
            "Epoch: 228 | Loss: 0.0005751793505623937 \n",
            "Epoch: 229 | Loss: 0.0005669180536642671 \n",
            "Epoch: 230 | Loss: 0.0005587639752775431 \n",
            "Epoch: 231 | Loss: 0.0005507208989001811 \n",
            "Epoch: 232 | Loss: 0.0005428172880783677 \n",
            "Epoch: 233 | Loss: 0.0005350145511329174 \n",
            "Epoch: 234 | Loss: 0.0005273277056403458 \n",
            "Epoch: 235 | Loss: 0.0005197468562982976 \n",
            "Epoch: 236 | Loss: 0.0005122800939716399 \n",
            "Epoch: 237 | Loss: 0.0005049170576967299 \n",
            "Epoch: 238 | Loss: 0.0004976604832336307 \n",
            "Epoch: 239 | Loss: 0.0004905092064291239 \n",
            "Epoch: 240 | Loss: 0.00048347172560170293 \n",
            "Epoch: 241 | Loss: 0.0004765022313222289 \n",
            "Epoch: 242 | Loss: 0.0004696670512203127 \n",
            "Epoch: 243 | Loss: 0.0004629107716027647 \n",
            "Epoch: 244 | Loss: 0.000456254871096462 \n",
            "Epoch: 245 | Loss: 0.0004496977780945599 \n",
            "Epoch: 246 | Loss: 0.00044324813643470407 \n",
            "Epoch: 247 | Loss: 0.00043687300058081746 \n",
            "Epoch: 248 | Loss: 0.00043059291783720255 \n",
            "Epoch: 249 | Loss: 0.000424407422542572 \n",
            "Epoch: 250 | Loss: 0.000418297597207129 \n",
            "Epoch: 251 | Loss: 0.00041229522321373224 \n",
            "Epoch: 252 | Loss: 0.0004063667147420347 \n",
            "Epoch: 253 | Loss: 0.0004005247901659459 \n",
            "Epoch: 254 | Loss: 0.00039477599784731865 \n",
            "Epoch: 255 | Loss: 0.00038909452268853784 \n",
            "Epoch: 256 | Loss: 0.00038350431714206934 \n",
            "Epoch: 257 | Loss: 0.0003780020633712411 \n",
            "Epoch: 258 | Loss: 0.0003725597925949842 \n",
            "Epoch: 259 | Loss: 0.0003672043385449797 \n",
            "Epoch: 260 | Loss: 0.0003619324415922165 \n",
            "Epoch: 261 | Loss: 0.0003567233507055789 \n",
            "Epoch: 262 | Loss: 0.0003516087308526039 \n",
            "Epoch: 263 | Loss: 0.0003465500194579363 \n",
            "Epoch: 264 | Loss: 0.00034156179754063487 \n",
            "Epoch: 265 | Loss: 0.00033665396040305495 \n",
            "Epoch: 266 | Loss: 0.00033181451726704836 \n",
            "Epoch: 267 | Loss: 0.0003270421293564141 \n",
            "Epoch: 268 | Loss: 0.0003223466337658465 \n",
            "Epoch: 269 | Loss: 0.00031771723297424614 \n",
            "Epoch: 270 | Loss: 0.000313149590510875 \n",
            "Epoch: 271 | Loss: 0.0003086446668021381 \n",
            "Epoch: 272 | Loss: 0.00030420630355365574 \n",
            "Epoch: 273 | Loss: 0.0002998413983732462 \n",
            "Epoch: 274 | Loss: 0.00029552701744250953 \n",
            "Epoch: 275 | Loss: 0.00029128906317055225 \n",
            "Epoch: 276 | Loss: 0.00028709106845781207 \n",
            "Epoch: 277 | Loss: 0.00028296609525568783 \n",
            "Epoch: 278 | Loss: 0.000278905441518873 \n",
            "Epoch: 279 | Loss: 0.00027489292551763356 \n",
            "Epoch: 280 | Loss: 0.00027094155666418374 \n",
            "Epoch: 281 | Loss: 0.0002670586691237986 \n",
            "Epoch: 282 | Loss: 0.0002632114046718925 \n",
            "Epoch: 283 | Loss: 0.0002594294783193618 \n",
            "Epoch: 284 | Loss: 0.00025570375146344304 \n",
            "Epoch: 285 | Loss: 0.00025202572578564286 \n",
            "Epoch: 286 | Loss: 0.00024840369587764144 \n",
            "Epoch: 287 | Loss: 0.00024483699235133827 \n",
            "Epoch: 288 | Loss: 0.0002413091715425253 \n",
            "Epoch: 289 | Loss: 0.00023785313533153385 \n",
            "Epoch: 290 | Loss: 0.00023442780366167426 \n",
            "Epoch: 291 | Loss: 0.00023106136359274387 \n",
            "Epoch: 292 | Loss: 0.00022774423996452242 \n",
            "Epoch: 293 | Loss: 0.000224463758058846 \n",
            "Epoch: 294 | Loss: 0.00022124340466689318 \n",
            "Epoch: 295 | Loss: 0.00021806391305290163 \n",
            "Epoch: 296 | Loss: 0.00021493043459486216 \n",
            "Epoch: 297 | Loss: 0.00021183869102969766 \n",
            "Epoch: 298 | Loss: 0.00020880167721770704 \n",
            "Epoch: 299 | Loss: 0.0002057987148873508 \n",
            "Epoch: 300 | Loss: 0.00020283857884351164 \n",
            "Epoch: 301 | Loss: 0.00019992623128928244 \n",
            "Epoch: 302 | Loss: 0.00019705860177055 \n",
            "Epoch: 303 | Loss: 0.00019421777687966824 \n",
            "Epoch: 304 | Loss: 0.00019142731616739184 \n",
            "Epoch: 305 | Loss: 0.00018867226026486605 \n",
            "Epoch: 306 | Loss: 0.0001859632902778685 \n",
            "Epoch: 307 | Loss: 0.000183298354386352 \n",
            "Epoch: 308 | Loss: 0.00018065849144477397 \n",
            "Epoch: 309 | Loss: 0.0001780622114893049 \n",
            "Epoch: 310 | Loss: 0.00017549630138091743 \n",
            "Epoch: 311 | Loss: 0.00017298034799750894 \n",
            "Epoch: 312 | Loss: 0.00017048865265678614 \n",
            "Epoch: 313 | Loss: 0.00016804139886517078 \n",
            "Epoch: 314 | Loss: 0.0001656223030295223 \n",
            "Epoch: 315 | Loss: 0.00016324564057867974 \n",
            "Epoch: 316 | Loss: 0.00016089426935650408 \n",
            "Epoch: 317 | Loss: 0.00015858776168897748 \n",
            "Epoch: 318 | Loss: 0.00015630338748451322 \n",
            "Epoch: 319 | Loss: 0.00015406010788865387 \n",
            "Epoch: 320 | Loss: 0.00015184987569227815 \n",
            "Epoch: 321 | Loss: 0.00014966240269131958 \n",
            "Epoch: 322 | Loss: 0.00014751088747289032 \n",
            "Epoch: 323 | Loss: 0.0001453970471629873 \n",
            "Epoch: 324 | Loss: 0.00014330403064377606 \n",
            "Epoch: 325 | Loss: 0.00014124692825134844 \n",
            "Epoch: 326 | Loss: 0.00013921430218033493 \n",
            "Epoch: 327 | Loss: 0.0001372144470224157 \n",
            "Epoch: 328 | Loss: 0.00013523684174288064 \n",
            "Epoch: 329 | Loss: 0.00013329373905435205 \n",
            "Epoch: 330 | Loss: 0.00013138132635504007 \n",
            "Epoch: 331 | Loss: 0.0001294983085244894 \n",
            "Epoch: 332 | Loss: 0.00012763083213940263 \n",
            "Epoch: 333 | Loss: 0.00012579650501720607 \n",
            "Epoch: 334 | Loss: 0.00012399099068716168 \n",
            "Epoch: 335 | Loss: 0.0001222052815137431 \n",
            "Epoch: 336 | Loss: 0.00012045069161104038 \n",
            "Epoch: 337 | Loss: 0.00011872336472151801 \n",
            "Epoch: 338 | Loss: 0.00011701481707859784 \n",
            "Epoch: 339 | Loss: 0.00011533020006027073 \n",
            "Epoch: 340 | Loss: 0.00011367452680133283 \n",
            "Epoch: 341 | Loss: 0.00011204313341295347 \n",
            "Epoch: 342 | Loss: 0.00011042812548112124 \n",
            "Epoch: 343 | Loss: 0.00010884212679229677 \n",
            "Epoch: 344 | Loss: 0.00010727513290476054 \n",
            "Epoch: 345 | Loss: 0.00010573443432804197 \n",
            "Epoch: 346 | Loss: 0.00010421378101455048 \n",
            "Epoch: 347 | Loss: 0.00010272211511619389 \n",
            "Epoch: 348 | Loss: 0.00010123712854692712 \n",
            "Epoch: 349 | Loss: 9.978210437111557e-05 \n",
            "Epoch: 350 | Loss: 9.83566278591752e-05 \n",
            "Epoch: 351 | Loss: 9.693942411104217e-05 \n",
            "Epoch: 352 | Loss: 9.554585994919762e-05 \n",
            "Epoch: 353 | Loss: 9.417535329703242e-05 \n",
            "Epoch: 354 | Loss: 9.28200752241537e-05 \n",
            "Epoch: 355 | Loss: 9.148602839559317e-05 \n",
            "Epoch: 356 | Loss: 9.016908006742597e-05 \n",
            "Epoch: 357 | Loss: 8.887346484698355e-05 \n",
            "Epoch: 358 | Loss: 8.759756019571796e-05 \n",
            "Epoch: 359 | Loss: 8.634007826913148e-05 \n",
            "Epoch: 360 | Loss: 8.510000770911574e-05 \n",
            "Epoch: 361 | Loss: 8.38752748677507e-05 \n",
            "Epoch: 362 | Loss: 8.267027442343533e-05 \n",
            "Epoch: 363 | Loss: 8.148342021740973e-05 \n",
            "Epoch: 364 | Loss: 8.031428296817467e-05 \n",
            "Epoch: 365 | Loss: 7.915410969872028e-05 \n",
            "Epoch: 366 | Loss: 7.801572792232037e-05 \n",
            "Epoch: 367 | Loss: 7.689935591770336e-05 \n",
            "Epoch: 368 | Loss: 7.579229713883251e-05 \n",
            "Epoch: 369 | Loss: 7.470582204405218e-05 \n",
            "Epoch: 370 | Loss: 7.362994074355811e-05 \n",
            "Epoch: 371 | Loss: 7.25727659300901e-05 \n",
            "Epoch: 372 | Loss: 7.152519538067281e-05 \n",
            "Epoch: 373 | Loss: 7.049964187899604e-05 \n",
            "Epoch: 374 | Loss: 6.948900409042835e-05 \n",
            "Epoch: 375 | Loss: 6.849312921985984e-05 \n",
            "Epoch: 376 | Loss: 6.75094488542527e-05 \n",
            "Epoch: 377 | Loss: 6.653550371993333e-05 \n",
            "Epoch: 378 | Loss: 6.557899178005755e-05 \n",
            "Epoch: 379 | Loss: 6.46371190669015e-05 \n",
            "Epoch: 380 | Loss: 6.370994378812611e-05 \n",
            "Epoch: 381 | Loss: 6.279061926761642e-05 \n",
            "Epoch: 382 | Loss: 6.189009582158178e-05 \n",
            "Epoch: 383 | Loss: 6.099948950577527e-05 \n",
            "Epoch: 384 | Loss: 6.0121867136331275e-05 \n",
            "Epoch: 385 | Loss: 5.9257545217406005e-05 \n",
            "Epoch: 386 | Loss: 5.840769154019654e-05 \n",
            "Epoch: 387 | Loss: 5.757017061114311e-05 \n",
            "Epoch: 388 | Loss: 5.6742406741250306e-05 \n",
            "Epoch: 389 | Loss: 5.592586239799857e-05 \n",
            "Epoch: 390 | Loss: 5.512473580893129e-05 \n",
            "Epoch: 391 | Loss: 5.432960097095929e-05 \n",
            "Epoch: 392 | Loss: 5.3550658776657656e-05 \n",
            "Epoch: 393 | Loss: 5.278241587802768e-05 \n",
            "Epoch: 394 | Loss: 5.2020772272953764e-05 \n",
            "Epoch: 395 | Loss: 5.127235999680124e-05 \n",
            "Epoch: 396 | Loss: 5.0534337788121775e-05 \n",
            "Epoch: 397 | Loss: 4.980925587005913e-05 \n",
            "Epoch: 398 | Loss: 4.90961319883354e-05 \n",
            "Epoch: 399 | Loss: 4.8389167204732075e-05 \n",
            "Epoch: 400 | Loss: 4.769856604980305e-05 \n",
            "Epoch: 401 | Loss: 4.701094803749584e-05 \n",
            "Epoch: 402 | Loss: 4.633583012036979e-05 \n",
            "Epoch: 403 | Loss: 4.566796269500628e-05 \n",
            "Epoch: 404 | Loss: 4.501449075178243e-05 \n",
            "Epoch: 405 | Loss: 4.436534800333902e-05 \n",
            "Epoch: 406 | Loss: 4.372802868601866e-05 \n",
            "Epoch: 407 | Loss: 4.3101044866489246e-05 \n",
            "Epoch: 408 | Loss: 4.2479536205064505e-05 \n",
            "Epoch: 409 | Loss: 4.1871760913636535e-05 \n",
            "Epoch: 410 | Loss: 4.126948624616489e-05 \n",
            "Epoch: 411 | Loss: 4.06756553275045e-05 \n",
            "Epoch: 412 | Loss: 4.0090730180963874e-05 \n",
            "Epoch: 413 | Loss: 3.951606777263805e-05 \n",
            "Epoch: 414 | Loss: 3.8947553548496217e-05 \n",
            "Epoch: 415 | Loss: 3.8389101973734796e-05 \n",
            "Epoch: 416 | Loss: 3.783504507737234e-05 \n",
            "Epoch: 417 | Loss: 3.728837691596709e-05 \n",
            "Epoch: 418 | Loss: 3.6753452150151134e-05 \n",
            "Epoch: 419 | Loss: 3.62271057383623e-05 \n",
            "Epoch: 420 | Loss: 3.5706994822248816e-05 \n",
            "Epoch: 421 | Loss: 3.519633173709735e-05 \n",
            "Epoch: 422 | Loss: 3.468849172350019e-05 \n",
            "Epoch: 423 | Loss: 3.41906379617285e-05 \n",
            "Epoch: 424 | Loss: 3.3698237530188635e-05 \n",
            "Epoch: 425 | Loss: 3.3213764254469424e-05 \n",
            "Epoch: 426 | Loss: 3.273462789366022e-05 \n",
            "Epoch: 427 | Loss: 3.2265081244986504e-05 \n",
            "Epoch: 428 | Loss: 3.180269413860515e-05 \n",
            "Epoch: 429 | Loss: 3.134478538413532e-05 \n",
            "Epoch: 430 | Loss: 3.089423989877105e-05 \n",
            "Epoch: 431 | Loss: 3.044760160264559e-05 \n",
            "Epoch: 432 | Loss: 3.000946708198171e-05 \n",
            "Epoch: 433 | Loss: 2.9581300623249263e-05 \n",
            "Epoch: 434 | Loss: 2.9154802177799866e-05 \n",
            "Epoch: 435 | Loss: 2.873607081710361e-05 \n",
            "Epoch: 436 | Loss: 2.8323162041488104e-05 \n",
            "Epoch: 437 | Loss: 2.7916770704905502e-05 \n",
            "Epoch: 438 | Loss: 2.751743159024045e-05 \n",
            "Epoch: 439 | Loss: 2.7117799618281424e-05 \n",
            "Epoch: 440 | Loss: 2.6732199330581352e-05 \n",
            "Epoch: 441 | Loss: 2.634623524500057e-05 \n",
            "Epoch: 442 | Loss: 2.596707417978905e-05 \n",
            "Epoch: 443 | Loss: 2.5596395062166266e-05 \n",
            "Epoch: 444 | Loss: 2.522532122384291e-05 \n",
            "Epoch: 445 | Loss: 2.4863187718437985e-05 \n",
            "Epoch: 446 | Loss: 2.450467763992492e-05 \n",
            "Epoch: 447 | Loss: 2.415305789327249e-05 \n",
            "Epoch: 448 | Loss: 2.380511796218343e-05 \n",
            "Epoch: 449 | Loss: 2.3465187041438185e-05 \n",
            "Epoch: 450 | Loss: 2.312644755875226e-05 \n",
            "Epoch: 451 | Loss: 2.2794474716647528e-05 \n",
            "Epoch: 452 | Loss: 2.2464901121566072e-05 \n",
            "Epoch: 453 | Loss: 2.2144016838865355e-05 \n",
            "Epoch: 454 | Loss: 2.1825982912559994e-05 \n",
            "Epoch: 455 | Loss: 2.151348780898843e-05 \n",
            "Epoch: 456 | Loss: 2.120298086083494e-05 \n",
            "Epoch: 457 | Loss: 2.0898980437777936e-05 \n",
            "Epoch: 458 | Loss: 2.0599158233380876e-05 \n",
            "Epoch: 459 | Loss: 2.030359610216692e-05 \n",
            "Epoch: 460 | Loss: 2.0012121240142733e-05 \n",
            "Epoch: 461 | Loss: 1.9722756405826658e-05 \n",
            "Epoch: 462 | Loss: 1.943895586009603e-05 \n",
            "Epoch: 463 | Loss: 1.915924804052338e-05 \n",
            "Epoch: 464 | Loss: 1.888459883048199e-05 \n",
            "Epoch: 465 | Loss: 1.861281089077238e-05 \n",
            "Epoch: 466 | Loss: 1.8345483113080263e-05 \n",
            "Epoch: 467 | Loss: 1.8081078451359645e-05 \n",
            "Epoch: 468 | Loss: 1.7821414076024666e-05 \n",
            "Epoch: 469 | Loss: 1.756692108756397e-05 \n",
            "Epoch: 470 | Loss: 1.73124426510185e-05 \n",
            "Epoch: 471 | Loss: 1.7064023268176243e-05 \n",
            "Epoch: 472 | Loss: 1.6817995856399648e-05 \n",
            "Epoch: 473 | Loss: 1.657836946833413e-05 \n",
            "Epoch: 474 | Loss: 1.6341404261766e-05 \n",
            "Epoch: 475 | Loss: 1.6104280803119764e-05 \n",
            "Epoch: 476 | Loss: 1.5874908058322035e-05 \n",
            "Epoch: 477 | Loss: 1.5645458915969357e-05 \n",
            "Epoch: 478 | Loss: 1.5422929209307767e-05 \n",
            "Epoch: 479 | Loss: 1.5200295820250176e-05 \n",
            "Epoch: 480 | Loss: 1.498074288974749e-05 \n",
            "Epoch: 481 | Loss: 1.4764019397262018e-05 \n",
            "Epoch: 482 | Loss: 1.4554302651959006e-05 \n",
            "Epoch: 483 | Loss: 1.4344657756737433e-05 \n",
            "Epoch: 484 | Loss: 1.4137081961962394e-05 \n",
            "Epoch: 485 | Loss: 1.393448928865837e-05 \n",
            "Epoch: 486 | Loss: 1.3733898413192946e-05 \n",
            "Epoch: 487 | Loss: 1.3536472579289693e-05 \n",
            "Epoch: 488 | Loss: 1.3343343198357616e-05 \n",
            "Epoch: 489 | Loss: 1.3149284313840326e-05 \n",
            "Epoch: 490 | Loss: 1.2961664651811589e-05 \n",
            "Epoch: 491 | Loss: 1.2776227777067106e-05 \n",
            "Epoch: 492 | Loss: 1.2591305676323827e-05 \n",
            "Epoch: 493 | Loss: 1.2410288945829961e-05 \n",
            "Epoch: 494 | Loss: 1.2233020243002102e-05 \n",
            "Epoch: 495 | Loss: 1.2056726518494543e-05 \n",
            "Epoch: 496 | Loss: 1.1883716069860384e-05 \n",
            "Epoch: 497 | Loss: 1.171205713035306e-05 \n",
            "Epoch: 498 | Loss: 1.1545200322871096e-05 \n",
            "Epoch: 499 | Loss: 1.1379934221622534e-05 \n",
            "Prediction (after training) 4 8.003877639770508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swKM4ZR8bsyF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}