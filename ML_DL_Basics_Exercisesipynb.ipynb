{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML/DL Basics Exercisesipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlSuE49XOSh3f2e93IN3tU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mindyng/ML-Keeping-it-FRESH/blob/master/ML_DL_Basics_Exercisesipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfdwo7-J1N5r"
      },
      "source": [
        "## Manual Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aki7lwRSX_km",
        "outputId": "41d520a0-5d53-49d7-f3e4-0353253128d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# List of weights/Mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    # Print the weights and initialize the loss\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    print(\"MSE=\", l_sum / 3)\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / 3)\n",
        "\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1MzKUwy1eYn"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Auto Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ2jAmLPYvW7",
        "outputId": "5eb23bd3-92be-4a36-fea4-d0cee2aaccbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2\n",
        "\n",
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4).item())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val) # 1) Forward pass\n",
        "        l = loss(y_pred, y_val) # 2) Compute loss\n",
        "        l.backward() # 3) Back propagation to update weights\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "# After training\n",
        "print(\"Prediction (after training)\",  4, forward(4).item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction (after training) 4 7.804864406585693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l8fyKkoZymg"
      },
      "source": [
        "## Pytorch-built Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-B4oIFA2yAH",
        "outputId": "677c0896-2d7f-4c3c-b0d5-241ab8072815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 25.841419219970703 \n",
            "Epoch: 1 | Loss: 11.512296676635742 \n",
            "Epoch: 2 | Loss: 5.13325834274292 \n",
            "Epoch: 3 | Loss: 2.293372631072998 \n",
            "Epoch: 4 | Loss: 1.0290179252624512 \n",
            "Epoch: 5 | Loss: 0.46604785323143005 \n",
            "Epoch: 6 | Loss: 0.2153146117925644 \n",
            "Epoch: 7 | Loss: 0.10358257591724396 \n",
            "Epoch: 8 | Loss: 0.053731758147478104 \n",
            "Epoch: 9 | Loss: 0.03142990916967392 \n",
            "Epoch: 10 | Loss: 0.021393848583102226 \n",
            "Epoch: 11 | Loss: 0.016819769516587257 \n",
            "Epoch: 12 | Loss: 0.014678658917546272 \n",
            "Epoch: 13 | Loss: 0.013622180558741093 \n",
            "Epoch: 14 | Loss: 0.013049978762865067 \n",
            "Epoch: 15 | Loss: 0.012694859877228737 \n",
            "Epoch: 16 | Loss: 0.012437851168215275 \n",
            "Epoch: 17 | Loss: 0.012225865386426449 \n",
            "Epoch: 18 | Loss: 0.012035384774208069 \n",
            "Epoch: 19 | Loss: 0.01185579877346754 \n",
            "Epoch: 20 | Loss: 0.011682509444653988 \n",
            "Epoch: 21 | Loss: 0.01151329930871725 \n",
            "Epoch: 22 | Loss: 0.011347251012921333 \n",
            "Epoch: 23 | Loss: 0.011183956637978554 \n",
            "Epoch: 24 | Loss: 0.011023059487342834 \n",
            "Epoch: 25 | Loss: 0.010864629410207272 \n",
            "Epoch: 26 | Loss: 0.010708415880799294 \n",
            "Epoch: 27 | Loss: 0.010554495267570019 \n",
            "Epoch: 28 | Loss: 0.010402850806713104 \n",
            "Epoch: 29 | Loss: 0.010253360494971275 \n",
            "Epoch: 30 | Loss: 0.010106019675731659 \n",
            "Epoch: 31 | Loss: 0.009960774332284927 \n",
            "Epoch: 32 | Loss: 0.009817583486437798 \n",
            "Epoch: 33 | Loss: 0.009676513262093067 \n",
            "Epoch: 34 | Loss: 0.009537412784993649 \n",
            "Epoch: 35 | Loss: 0.009400369599461555 \n",
            "Epoch: 36 | Loss: 0.009265268221497536 \n",
            "Epoch: 37 | Loss: 0.009132103063166142 \n",
            "Epoch: 38 | Loss: 0.009000842459499836 \n",
            "Epoch: 39 | Loss: 0.008871525526046753 \n",
            "Epoch: 40 | Loss: 0.00874399021267891 \n",
            "Epoch: 41 | Loss: 0.008618341758847237 \n",
            "Epoch: 42 | Loss: 0.00849449634552002 \n",
            "Epoch: 43 | Loss: 0.008372411131858826 \n",
            "Epoch: 44 | Loss: 0.008252106606960297 \n",
            "Epoch: 45 | Loss: 0.00813351385295391 \n",
            "Epoch: 46 | Loss: 0.008016594685614109 \n",
            "Epoch: 47 | Loss: 0.007901374250650406 \n",
            "Epoch: 48 | Loss: 0.007787827402353287 \n",
            "Epoch: 49 | Loss: 0.0076758842915296555 \n",
            "Epoch: 50 | Loss: 0.007565600797533989 \n",
            "Epoch: 51 | Loss: 0.007456868886947632 \n",
            "Epoch: 52 | Loss: 0.007349706254899502 \n",
            "Epoch: 53 | Loss: 0.007244037464261055 \n",
            "Epoch: 54 | Loss: 0.007140002679079771 \n",
            "Epoch: 55 | Loss: 0.007037351839244366 \n",
            "Epoch: 56 | Loss: 0.006936248857527971 \n",
            "Epoch: 57 | Loss: 0.006836533080786467 \n",
            "Epoch: 58 | Loss: 0.006738248281180859 \n",
            "Epoch: 59 | Loss: 0.006641423795372248 \n",
            "Epoch: 60 | Loss: 0.0065459939651191235 \n",
            "Epoch: 61 | Loss: 0.0064518870785832405 \n",
            "Epoch: 62 | Loss: 0.006359162274748087 \n",
            "Epoch: 63 | Loss: 0.0062678102403879166 \n",
            "Epoch: 64 | Loss: 0.006177715957164764 \n",
            "Epoch: 65 | Loss: 0.006088935770094395 \n",
            "Epoch: 66 | Loss: 0.006001422181725502 \n",
            "Epoch: 67 | Loss: 0.005915192887187004 \n",
            "Epoch: 68 | Loss: 0.005830186884850264 \n",
            "Epoch: 69 | Loss: 0.005746375769376755 \n",
            "Epoch: 70 | Loss: 0.005663829855620861 \n",
            "Epoch: 71 | Loss: 0.005582405254244804 \n",
            "Epoch: 72 | Loss: 0.005502146668732166 \n",
            "Epoch: 73 | Loss: 0.005423065274953842 \n",
            "Epoch: 74 | Loss: 0.005345171317458153 \n",
            "Epoch: 75 | Loss: 0.005268352571874857 \n",
            "Epoch: 76 | Loss: 0.005192632786929607 \n",
            "Epoch: 77 | Loss: 0.005117993801832199 \n",
            "Epoch: 78 | Loss: 0.005044472403824329 \n",
            "Epoch: 79 | Loss: 0.004971951246261597 \n",
            "Epoch: 80 | Loss: 0.004900504369288683 \n",
            "Epoch: 81 | Loss: 0.004830085672438145 \n",
            "Epoch: 82 | Loss: 0.004760655574500561 \n",
            "Epoch: 83 | Loss: 0.004692266695201397 \n",
            "Epoch: 84 | Loss: 0.004624806810170412 \n",
            "Epoch: 85 | Loss: 0.004558349959552288 \n",
            "Epoch: 86 | Loss: 0.0044928123243153095 \n",
            "Epoch: 87 | Loss: 0.0044282409362494946 \n",
            "Epoch: 88 | Loss: 0.004364633932709694 \n",
            "Epoch: 89 | Loss: 0.004301910754293203 \n",
            "Epoch: 90 | Loss: 0.00424007885158062 \n",
            "Epoch: 91 | Loss: 0.004179127048701048 \n",
            "Epoch: 92 | Loss: 0.004119069315493107 \n",
            "Epoch: 93 | Loss: 0.004059883765876293 \n",
            "Epoch: 94 | Loss: 0.004001551307737827 \n",
            "Epoch: 95 | Loss: 0.003944022580981255 \n",
            "Epoch: 96 | Loss: 0.0038873341400176287 \n",
            "Epoch: 97 | Loss: 0.003831432666629553 \n",
            "Epoch: 98 | Loss: 0.0037763877771794796 \n",
            "Epoch: 99 | Loss: 0.003722127992659807 \n",
            "Epoch: 100 | Loss: 0.003668640973046422 \n",
            "Epoch: 101 | Loss: 0.0036158873699605465 \n",
            "Epoch: 102 | Loss: 0.0035639600828289986 \n",
            "Epoch: 103 | Loss: 0.003512746188789606 \n",
            "Epoch: 104 | Loss: 0.003462224267423153 \n",
            "Epoch: 105 | Loss: 0.0034124981611967087 \n",
            "Epoch: 106 | Loss: 0.0033634451683610678 \n",
            "Epoch: 107 | Loss: 0.0033151106908917427 \n",
            "Epoch: 108 | Loss: 0.003267488442361355 \n",
            "Epoch: 109 | Loss: 0.0032205369789153337 \n",
            "Epoch: 110 | Loss: 0.0031742220744490623 \n",
            "Epoch: 111 | Loss: 0.0031285970471799374 \n",
            "Epoch: 112 | Loss: 0.003083650954067707 \n",
            "Epoch: 113 | Loss: 0.003039306029677391 \n",
            "Epoch: 114 | Loss: 0.002995652612298727 \n",
            "Epoch: 115 | Loss: 0.002952587092295289 \n",
            "Epoch: 116 | Loss: 0.002910182811319828 \n",
            "Epoch: 117 | Loss: 0.002868358977138996 \n",
            "Epoch: 118 | Loss: 0.002827121177688241 \n",
            "Epoch: 119 | Loss: 0.0027865057345479727 \n",
            "Epoch: 120 | Loss: 0.002746444894000888 \n",
            "Epoch: 121 | Loss: 0.002706956584006548 \n",
            "Epoch: 122 | Loss: 0.0026680787559598684 \n",
            "Epoch: 123 | Loss: 0.0026297385338693857 \n",
            "Epoch: 124 | Loss: 0.002591937780380249 \n",
            "Epoch: 125 | Loss: 0.0025546811521053314 \n",
            "Epoch: 126 | Loss: 0.002517971210181713 \n",
            "Epoch: 127 | Loss: 0.0024817832745611668 \n",
            "Epoch: 128 | Loss: 0.002446107566356659 \n",
            "Epoch: 129 | Loss: 0.002410934306681156 \n",
            "Epoch: 130 | Loss: 0.0023762849159538746 \n",
            "Epoch: 131 | Loss: 0.002342149382457137 \n",
            "Epoch: 132 | Loss: 0.0023084941785782576 \n",
            "Epoch: 133 | Loss: 0.002275322563946247 \n",
            "Epoch: 134 | Loss: 0.0022426042705774307 \n",
            "Epoch: 135 | Loss: 0.002210381207987666 \n",
            "Epoch: 136 | Loss: 0.002178613794967532 \n",
            "Epoch: 137 | Loss: 0.0021473108790814877 \n",
            "Epoch: 138 | Loss: 0.0021164698991924524 \n",
            "Epoch: 139 | Loss: 0.002086025895550847 \n",
            "Epoch: 140 | Loss: 0.0020560473203659058 \n",
            "Epoch: 141 | Loss: 0.0020265139173716307 \n",
            "Epoch: 142 | Loss: 0.00199737586081028 \n",
            "Epoch: 143 | Loss: 0.0019686680752784014 \n",
            "Epoch: 144 | Loss: 0.0019403770565986633 \n",
            "Epoch: 145 | Loss: 0.0019124990794807673 \n",
            "Epoch: 146 | Loss: 0.001884995843283832 \n",
            "Epoch: 147 | Loss: 0.001857912284322083 \n",
            "Epoch: 148 | Loss: 0.0018312076572328806 \n",
            "Epoch: 149 | Loss: 0.0018048925558105111 \n",
            "Epoch: 150 | Loss: 0.001778960577212274 \n",
            "Epoch: 151 | Loss: 0.0017534024082124233 \n",
            "Epoch: 152 | Loss: 0.0017282019834965467 \n",
            "Epoch: 153 | Loss: 0.0017033532494679093 \n",
            "Epoch: 154 | Loss: 0.0016788928769528866 \n",
            "Epoch: 155 | Loss: 0.0016547406557947397 \n",
            "Epoch: 156 | Loss: 0.0016309770289808512 \n",
            "Epoch: 157 | Loss: 0.0016075300518423319 \n",
            "Epoch: 158 | Loss: 0.0015844331355765462 \n",
            "Epoch: 159 | Loss: 0.0015616723103448749 \n",
            "Epoch: 160 | Loss: 0.0015392097411677241 \n",
            "Epoch: 161 | Loss: 0.001517096534371376 \n",
            "Epoch: 162 | Loss: 0.0014953108038753271 \n",
            "Epoch: 163 | Loss: 0.0014738133177161217 \n",
            "Epoch: 164 | Loss: 0.0014526156010106206 \n",
            "Epoch: 165 | Loss: 0.0014317571185529232 \n",
            "Epoch: 166 | Loss: 0.0014111697673797607 \n",
            "Epoch: 167 | Loss: 0.0013908692635595798 \n",
            "Epoch: 168 | Loss: 0.0013708891347050667 \n",
            "Epoch: 169 | Loss: 0.001351198647171259 \n",
            "Epoch: 170 | Loss: 0.0013317686971276999 \n",
            "Epoch: 171 | Loss: 0.001312642591074109 \n",
            "Epoch: 172 | Loss: 0.001293763518333435 \n",
            "Epoch: 173 | Loss: 0.001275177812203765 \n",
            "Epoch: 174 | Loss: 0.001256840187124908 \n",
            "Epoch: 175 | Loss: 0.0012387847527861595 \n",
            "Epoch: 176 | Loss: 0.0012209948617964983 \n",
            "Epoch: 177 | Loss: 0.0012034408282488585 \n",
            "Epoch: 178 | Loss: 0.0011861308012157679 \n",
            "Epoch: 179 | Loss: 0.0011690864339470863 \n",
            "Epoch: 180 | Loss: 0.0011522966669872403 \n",
            "Epoch: 181 | Loss: 0.0011357174953445792 \n",
            "Epoch: 182 | Loss: 0.0011194043327122927 \n",
            "Epoch: 183 | Loss: 0.0011033376213163137 \n",
            "Epoch: 184 | Loss: 0.0010874455329030752 \n",
            "Epoch: 185 | Loss: 0.0010718372650444508 \n",
            "Epoch: 186 | Loss: 0.0010564362164586782 \n",
            "Epoch: 187 | Loss: 0.00104123679921031 \n",
            "Epoch: 188 | Loss: 0.0010262945434078574 \n",
            "Epoch: 189 | Loss: 0.0010115231852978468 \n",
            "Epoch: 190 | Loss: 0.0009970057290047407 \n",
            "Epoch: 191 | Loss: 0.0009826780296862125 \n",
            "Epoch: 192 | Loss: 0.0009685445111244917 \n",
            "Epoch: 193 | Loss: 0.0009546256624162197 \n",
            "Epoch: 194 | Loss: 0.0009409082122147083 \n",
            "Epoch: 195 | Loss: 0.0009273893665522337 \n",
            "Epoch: 196 | Loss: 0.0009140552720054984 \n",
            "Epoch: 197 | Loss: 0.0009009127970784903 \n",
            "Epoch: 198 | Loss: 0.0008879712549969554 \n",
            "Epoch: 199 | Loss: 0.0008752109715715051 \n",
            "Epoch: 200 | Loss: 0.0008626336930319667 \n",
            "Epoch: 201 | Loss: 0.0008502346463501453 \n",
            "Epoch: 202 | Loss: 0.000838021282106638 \n",
            "Epoch: 203 | Loss: 0.0008259824826382101 \n",
            "Epoch: 204 | Loss: 0.0008141069556586444 \n",
            "Epoch: 205 | Loss: 0.0008024047710932791 \n",
            "Epoch: 206 | Loss: 0.0007908657426014543 \n",
            "Epoch: 207 | Loss: 0.0007795239216648042 \n",
            "Epoch: 208 | Loss: 0.0007683028234168887 \n",
            "Epoch: 209 | Loss: 0.000757262168917805 \n",
            "Epoch: 210 | Loss: 0.0007463847869075835 \n",
            "Epoch: 211 | Loss: 0.0007356492569670081 \n",
            "Epoch: 212 | Loss: 0.0007250861381180584 \n",
            "Epoch: 213 | Loss: 0.0007146709831431508 \n",
            "Epoch: 214 | Loss: 0.0007043974474072456 \n",
            "Epoch: 215 | Loss: 0.0006942665204405785 \n",
            "Epoch: 216 | Loss: 0.0006842957227490842 \n",
            "Epoch: 217 | Loss: 0.0006744550773873925 \n",
            "Epoch: 218 | Loss: 0.0006647569825872779 \n",
            "Epoch: 219 | Loss: 0.0006552072591148317 \n",
            "Epoch: 220 | Loss: 0.0006457938579842448 \n",
            "Epoch: 221 | Loss: 0.0006365115405060351 \n",
            "Epoch: 222 | Loss: 0.0006273657199926674 \n",
            "Epoch: 223 | Loss: 0.0006183431250974536 \n",
            "Epoch: 224 | Loss: 0.0006094677373766899 \n",
            "Epoch: 225 | Loss: 0.0006007135380059481 \n",
            "Epoch: 226 | Loss: 0.0005920756375417113 \n",
            "Epoch: 227 | Loss: 0.0005835649790242314 \n",
            "Epoch: 228 | Loss: 0.0005751793505623937 \n",
            "Epoch: 229 | Loss: 0.0005669180536642671 \n",
            "Epoch: 230 | Loss: 0.0005587639752775431 \n",
            "Epoch: 231 | Loss: 0.0005507208989001811 \n",
            "Epoch: 232 | Loss: 0.0005428172880783677 \n",
            "Epoch: 233 | Loss: 0.0005350145511329174 \n",
            "Epoch: 234 | Loss: 0.0005273277056403458 \n",
            "Epoch: 235 | Loss: 0.0005197468562982976 \n",
            "Epoch: 236 | Loss: 0.0005122800939716399 \n",
            "Epoch: 237 | Loss: 0.0005049170576967299 \n",
            "Epoch: 238 | Loss: 0.0004976604832336307 \n",
            "Epoch: 239 | Loss: 0.0004905092064291239 \n",
            "Epoch: 240 | Loss: 0.00048347172560170293 \n",
            "Epoch: 241 | Loss: 0.0004765022313222289 \n",
            "Epoch: 242 | Loss: 0.0004696670512203127 \n",
            "Epoch: 243 | Loss: 0.0004629107716027647 \n",
            "Epoch: 244 | Loss: 0.000456254871096462 \n",
            "Epoch: 245 | Loss: 0.0004496977780945599 \n",
            "Epoch: 246 | Loss: 0.00044324813643470407 \n",
            "Epoch: 247 | Loss: 0.00043687300058081746 \n",
            "Epoch: 248 | Loss: 0.00043059291783720255 \n",
            "Epoch: 249 | Loss: 0.000424407422542572 \n",
            "Epoch: 250 | Loss: 0.000418297597207129 \n",
            "Epoch: 251 | Loss: 0.00041229522321373224 \n",
            "Epoch: 252 | Loss: 0.0004063667147420347 \n",
            "Epoch: 253 | Loss: 0.0004005247901659459 \n",
            "Epoch: 254 | Loss: 0.00039477599784731865 \n",
            "Epoch: 255 | Loss: 0.00038909452268853784 \n",
            "Epoch: 256 | Loss: 0.00038350431714206934 \n",
            "Epoch: 257 | Loss: 0.0003780020633712411 \n",
            "Epoch: 258 | Loss: 0.0003725597925949842 \n",
            "Epoch: 259 | Loss: 0.0003672043385449797 \n",
            "Epoch: 260 | Loss: 0.0003619324415922165 \n",
            "Epoch: 261 | Loss: 0.0003567233507055789 \n",
            "Epoch: 262 | Loss: 0.0003516087308526039 \n",
            "Epoch: 263 | Loss: 0.0003465500194579363 \n",
            "Epoch: 264 | Loss: 0.00034156179754063487 \n",
            "Epoch: 265 | Loss: 0.00033665396040305495 \n",
            "Epoch: 266 | Loss: 0.00033181451726704836 \n",
            "Epoch: 267 | Loss: 0.0003270421293564141 \n",
            "Epoch: 268 | Loss: 0.0003223466337658465 \n",
            "Epoch: 269 | Loss: 0.00031771723297424614 \n",
            "Epoch: 270 | Loss: 0.000313149590510875 \n",
            "Epoch: 271 | Loss: 0.0003086446668021381 \n",
            "Epoch: 272 | Loss: 0.00030420630355365574 \n",
            "Epoch: 273 | Loss: 0.0002998413983732462 \n",
            "Epoch: 274 | Loss: 0.00029552701744250953 \n",
            "Epoch: 275 | Loss: 0.00029128906317055225 \n",
            "Epoch: 276 | Loss: 0.00028709106845781207 \n",
            "Epoch: 277 | Loss: 0.00028296609525568783 \n",
            "Epoch: 278 | Loss: 0.000278905441518873 \n",
            "Epoch: 279 | Loss: 0.00027489292551763356 \n",
            "Epoch: 280 | Loss: 0.00027094155666418374 \n",
            "Epoch: 281 | Loss: 0.0002670586691237986 \n",
            "Epoch: 282 | Loss: 0.0002632114046718925 \n",
            "Epoch: 283 | Loss: 0.0002594294783193618 \n",
            "Epoch: 284 | Loss: 0.00025570375146344304 \n",
            "Epoch: 285 | Loss: 0.00025202572578564286 \n",
            "Epoch: 286 | Loss: 0.00024840369587764144 \n",
            "Epoch: 287 | Loss: 0.00024483699235133827 \n",
            "Epoch: 288 | Loss: 0.0002413091715425253 \n",
            "Epoch: 289 | Loss: 0.00023785313533153385 \n",
            "Epoch: 290 | Loss: 0.00023442780366167426 \n",
            "Epoch: 291 | Loss: 0.00023106136359274387 \n",
            "Epoch: 292 | Loss: 0.00022774423996452242 \n",
            "Epoch: 293 | Loss: 0.000224463758058846 \n",
            "Epoch: 294 | Loss: 0.00022124340466689318 \n",
            "Epoch: 295 | Loss: 0.00021806391305290163 \n",
            "Epoch: 296 | Loss: 0.00021493043459486216 \n",
            "Epoch: 297 | Loss: 0.00021183869102969766 \n",
            "Epoch: 298 | Loss: 0.00020880167721770704 \n",
            "Epoch: 299 | Loss: 0.0002057987148873508 \n",
            "Epoch: 300 | Loss: 0.00020283857884351164 \n",
            "Epoch: 301 | Loss: 0.00019992623128928244 \n",
            "Epoch: 302 | Loss: 0.00019705860177055 \n",
            "Epoch: 303 | Loss: 0.00019421777687966824 \n",
            "Epoch: 304 | Loss: 0.00019142731616739184 \n",
            "Epoch: 305 | Loss: 0.00018867226026486605 \n",
            "Epoch: 306 | Loss: 0.0001859632902778685 \n",
            "Epoch: 307 | Loss: 0.000183298354386352 \n",
            "Epoch: 308 | Loss: 0.00018065849144477397 \n",
            "Epoch: 309 | Loss: 0.0001780622114893049 \n",
            "Epoch: 310 | Loss: 0.00017549630138091743 \n",
            "Epoch: 311 | Loss: 0.00017298034799750894 \n",
            "Epoch: 312 | Loss: 0.00017048865265678614 \n",
            "Epoch: 313 | Loss: 0.00016804139886517078 \n",
            "Epoch: 314 | Loss: 0.0001656223030295223 \n",
            "Epoch: 315 | Loss: 0.00016324564057867974 \n",
            "Epoch: 316 | Loss: 0.00016089426935650408 \n",
            "Epoch: 317 | Loss: 0.00015858776168897748 \n",
            "Epoch: 318 | Loss: 0.00015630338748451322 \n",
            "Epoch: 319 | Loss: 0.00015406010788865387 \n",
            "Epoch: 320 | Loss: 0.00015184987569227815 \n",
            "Epoch: 321 | Loss: 0.00014966240269131958 \n",
            "Epoch: 322 | Loss: 0.00014751088747289032 \n",
            "Epoch: 323 | Loss: 0.0001453970471629873 \n",
            "Epoch: 324 | Loss: 0.00014330403064377606 \n",
            "Epoch: 325 | Loss: 0.00014124692825134844 \n",
            "Epoch: 326 | Loss: 0.00013921430218033493 \n",
            "Epoch: 327 | Loss: 0.0001372144470224157 \n",
            "Epoch: 328 | Loss: 0.00013523684174288064 \n",
            "Epoch: 329 | Loss: 0.00013329373905435205 \n",
            "Epoch: 330 | Loss: 0.00013138132635504007 \n",
            "Epoch: 331 | Loss: 0.0001294983085244894 \n",
            "Epoch: 332 | Loss: 0.00012763083213940263 \n",
            "Epoch: 333 | Loss: 0.00012579650501720607 \n",
            "Epoch: 334 | Loss: 0.00012399099068716168 \n",
            "Epoch: 335 | Loss: 0.0001222052815137431 \n",
            "Epoch: 336 | Loss: 0.00012045069161104038 \n",
            "Epoch: 337 | Loss: 0.00011872336472151801 \n",
            "Epoch: 338 | Loss: 0.00011701481707859784 \n",
            "Epoch: 339 | Loss: 0.00011533020006027073 \n",
            "Epoch: 340 | Loss: 0.00011367452680133283 \n",
            "Epoch: 341 | Loss: 0.00011204313341295347 \n",
            "Epoch: 342 | Loss: 0.00011042812548112124 \n",
            "Epoch: 343 | Loss: 0.00010884212679229677 \n",
            "Epoch: 344 | Loss: 0.00010727513290476054 \n",
            "Epoch: 345 | Loss: 0.00010573443432804197 \n",
            "Epoch: 346 | Loss: 0.00010421378101455048 \n",
            "Epoch: 347 | Loss: 0.00010272211511619389 \n",
            "Epoch: 348 | Loss: 0.00010123712854692712 \n",
            "Epoch: 349 | Loss: 9.978210437111557e-05 \n",
            "Epoch: 350 | Loss: 9.83566278591752e-05 \n",
            "Epoch: 351 | Loss: 9.693942411104217e-05 \n",
            "Epoch: 352 | Loss: 9.554585994919762e-05 \n",
            "Epoch: 353 | Loss: 9.417535329703242e-05 \n",
            "Epoch: 354 | Loss: 9.28200752241537e-05 \n",
            "Epoch: 355 | Loss: 9.148602839559317e-05 \n",
            "Epoch: 356 | Loss: 9.016908006742597e-05 \n",
            "Epoch: 357 | Loss: 8.887346484698355e-05 \n",
            "Epoch: 358 | Loss: 8.759756019571796e-05 \n",
            "Epoch: 359 | Loss: 8.634007826913148e-05 \n",
            "Epoch: 360 | Loss: 8.510000770911574e-05 \n",
            "Epoch: 361 | Loss: 8.38752748677507e-05 \n",
            "Epoch: 362 | Loss: 8.267027442343533e-05 \n",
            "Epoch: 363 | Loss: 8.148342021740973e-05 \n",
            "Epoch: 364 | Loss: 8.031428296817467e-05 \n",
            "Epoch: 365 | Loss: 7.915410969872028e-05 \n",
            "Epoch: 366 | Loss: 7.801572792232037e-05 \n",
            "Epoch: 367 | Loss: 7.689935591770336e-05 \n",
            "Epoch: 368 | Loss: 7.579229713883251e-05 \n",
            "Epoch: 369 | Loss: 7.470582204405218e-05 \n",
            "Epoch: 370 | Loss: 7.362994074355811e-05 \n",
            "Epoch: 371 | Loss: 7.25727659300901e-05 \n",
            "Epoch: 372 | Loss: 7.152519538067281e-05 \n",
            "Epoch: 373 | Loss: 7.049964187899604e-05 \n",
            "Epoch: 374 | Loss: 6.948900409042835e-05 \n",
            "Epoch: 375 | Loss: 6.849312921985984e-05 \n",
            "Epoch: 376 | Loss: 6.75094488542527e-05 \n",
            "Epoch: 377 | Loss: 6.653550371993333e-05 \n",
            "Epoch: 378 | Loss: 6.557899178005755e-05 \n",
            "Epoch: 379 | Loss: 6.46371190669015e-05 \n",
            "Epoch: 380 | Loss: 6.370994378812611e-05 \n",
            "Epoch: 381 | Loss: 6.279061926761642e-05 \n",
            "Epoch: 382 | Loss: 6.189009582158178e-05 \n",
            "Epoch: 383 | Loss: 6.099948950577527e-05 \n",
            "Epoch: 384 | Loss: 6.0121867136331275e-05 \n",
            "Epoch: 385 | Loss: 5.9257545217406005e-05 \n",
            "Epoch: 386 | Loss: 5.840769154019654e-05 \n",
            "Epoch: 387 | Loss: 5.757017061114311e-05 \n",
            "Epoch: 388 | Loss: 5.6742406741250306e-05 \n",
            "Epoch: 389 | Loss: 5.592586239799857e-05 \n",
            "Epoch: 390 | Loss: 5.512473580893129e-05 \n",
            "Epoch: 391 | Loss: 5.432960097095929e-05 \n",
            "Epoch: 392 | Loss: 5.3550658776657656e-05 \n",
            "Epoch: 393 | Loss: 5.278241587802768e-05 \n",
            "Epoch: 394 | Loss: 5.2020772272953764e-05 \n",
            "Epoch: 395 | Loss: 5.127235999680124e-05 \n",
            "Epoch: 396 | Loss: 5.0534337788121775e-05 \n",
            "Epoch: 397 | Loss: 4.980925587005913e-05 \n",
            "Epoch: 398 | Loss: 4.90961319883354e-05 \n",
            "Epoch: 399 | Loss: 4.8389167204732075e-05 \n",
            "Epoch: 400 | Loss: 4.769856604980305e-05 \n",
            "Epoch: 401 | Loss: 4.701094803749584e-05 \n",
            "Epoch: 402 | Loss: 4.633583012036979e-05 \n",
            "Epoch: 403 | Loss: 4.566796269500628e-05 \n",
            "Epoch: 404 | Loss: 4.501449075178243e-05 \n",
            "Epoch: 405 | Loss: 4.436534800333902e-05 \n",
            "Epoch: 406 | Loss: 4.372802868601866e-05 \n",
            "Epoch: 407 | Loss: 4.3101044866489246e-05 \n",
            "Epoch: 408 | Loss: 4.2479536205064505e-05 \n",
            "Epoch: 409 | Loss: 4.1871760913636535e-05 \n",
            "Epoch: 410 | Loss: 4.126948624616489e-05 \n",
            "Epoch: 411 | Loss: 4.06756553275045e-05 \n",
            "Epoch: 412 | Loss: 4.0090730180963874e-05 \n",
            "Epoch: 413 | Loss: 3.951606777263805e-05 \n",
            "Epoch: 414 | Loss: 3.8947553548496217e-05 \n",
            "Epoch: 415 | Loss: 3.8389101973734796e-05 \n",
            "Epoch: 416 | Loss: 3.783504507737234e-05 \n",
            "Epoch: 417 | Loss: 3.728837691596709e-05 \n",
            "Epoch: 418 | Loss: 3.6753452150151134e-05 \n",
            "Epoch: 419 | Loss: 3.62271057383623e-05 \n",
            "Epoch: 420 | Loss: 3.5706994822248816e-05 \n",
            "Epoch: 421 | Loss: 3.519633173709735e-05 \n",
            "Epoch: 422 | Loss: 3.468849172350019e-05 \n",
            "Epoch: 423 | Loss: 3.41906379617285e-05 \n",
            "Epoch: 424 | Loss: 3.3698237530188635e-05 \n",
            "Epoch: 425 | Loss: 3.3213764254469424e-05 \n",
            "Epoch: 426 | Loss: 3.273462789366022e-05 \n",
            "Epoch: 427 | Loss: 3.2265081244986504e-05 \n",
            "Epoch: 428 | Loss: 3.180269413860515e-05 \n",
            "Epoch: 429 | Loss: 3.134478538413532e-05 \n",
            "Epoch: 430 | Loss: 3.089423989877105e-05 \n",
            "Epoch: 431 | Loss: 3.044760160264559e-05 \n",
            "Epoch: 432 | Loss: 3.000946708198171e-05 \n",
            "Epoch: 433 | Loss: 2.9581300623249263e-05 \n",
            "Epoch: 434 | Loss: 2.9154802177799866e-05 \n",
            "Epoch: 435 | Loss: 2.873607081710361e-05 \n",
            "Epoch: 436 | Loss: 2.8323162041488104e-05 \n",
            "Epoch: 437 | Loss: 2.7916770704905502e-05 \n",
            "Epoch: 438 | Loss: 2.751743159024045e-05 \n",
            "Epoch: 439 | Loss: 2.7117799618281424e-05 \n",
            "Epoch: 440 | Loss: 2.6732199330581352e-05 \n",
            "Epoch: 441 | Loss: 2.634623524500057e-05 \n",
            "Epoch: 442 | Loss: 2.596707417978905e-05 \n",
            "Epoch: 443 | Loss: 2.5596395062166266e-05 \n",
            "Epoch: 444 | Loss: 2.522532122384291e-05 \n",
            "Epoch: 445 | Loss: 2.4863187718437985e-05 \n",
            "Epoch: 446 | Loss: 2.450467763992492e-05 \n",
            "Epoch: 447 | Loss: 2.415305789327249e-05 \n",
            "Epoch: 448 | Loss: 2.380511796218343e-05 \n",
            "Epoch: 449 | Loss: 2.3465187041438185e-05 \n",
            "Epoch: 450 | Loss: 2.312644755875226e-05 \n",
            "Epoch: 451 | Loss: 2.2794474716647528e-05 \n",
            "Epoch: 452 | Loss: 2.2464901121566072e-05 \n",
            "Epoch: 453 | Loss: 2.2144016838865355e-05 \n",
            "Epoch: 454 | Loss: 2.1825982912559994e-05 \n",
            "Epoch: 455 | Loss: 2.151348780898843e-05 \n",
            "Epoch: 456 | Loss: 2.120298086083494e-05 \n",
            "Epoch: 457 | Loss: 2.0898980437777936e-05 \n",
            "Epoch: 458 | Loss: 2.0599158233380876e-05 \n",
            "Epoch: 459 | Loss: 2.030359610216692e-05 \n",
            "Epoch: 460 | Loss: 2.0012121240142733e-05 \n",
            "Epoch: 461 | Loss: 1.9722756405826658e-05 \n",
            "Epoch: 462 | Loss: 1.943895586009603e-05 \n",
            "Epoch: 463 | Loss: 1.915924804052338e-05 \n",
            "Epoch: 464 | Loss: 1.888459883048199e-05 \n",
            "Epoch: 465 | Loss: 1.861281089077238e-05 \n",
            "Epoch: 466 | Loss: 1.8345483113080263e-05 \n",
            "Epoch: 467 | Loss: 1.8081078451359645e-05 \n",
            "Epoch: 468 | Loss: 1.7821414076024666e-05 \n",
            "Epoch: 469 | Loss: 1.756692108756397e-05 \n",
            "Epoch: 470 | Loss: 1.73124426510185e-05 \n",
            "Epoch: 471 | Loss: 1.7064023268176243e-05 \n",
            "Epoch: 472 | Loss: 1.6817995856399648e-05 \n",
            "Epoch: 473 | Loss: 1.657836946833413e-05 \n",
            "Epoch: 474 | Loss: 1.6341404261766e-05 \n",
            "Epoch: 475 | Loss: 1.6104280803119764e-05 \n",
            "Epoch: 476 | Loss: 1.5874908058322035e-05 \n",
            "Epoch: 477 | Loss: 1.5645458915969357e-05 \n",
            "Epoch: 478 | Loss: 1.5422929209307767e-05 \n",
            "Epoch: 479 | Loss: 1.5200295820250176e-05 \n",
            "Epoch: 480 | Loss: 1.498074288974749e-05 \n",
            "Epoch: 481 | Loss: 1.4764019397262018e-05 \n",
            "Epoch: 482 | Loss: 1.4554302651959006e-05 \n",
            "Epoch: 483 | Loss: 1.4344657756737433e-05 \n",
            "Epoch: 484 | Loss: 1.4137081961962394e-05 \n",
            "Epoch: 485 | Loss: 1.393448928865837e-05 \n",
            "Epoch: 486 | Loss: 1.3733898413192946e-05 \n",
            "Epoch: 487 | Loss: 1.3536472579289693e-05 \n",
            "Epoch: 488 | Loss: 1.3343343198357616e-05 \n",
            "Epoch: 489 | Loss: 1.3149284313840326e-05 \n",
            "Epoch: 490 | Loss: 1.2961664651811589e-05 \n",
            "Epoch: 491 | Loss: 1.2776227777067106e-05 \n",
            "Epoch: 492 | Loss: 1.2591305676323827e-05 \n",
            "Epoch: 493 | Loss: 1.2410288945829961e-05 \n",
            "Epoch: 494 | Loss: 1.2233020243002102e-05 \n",
            "Epoch: 495 | Loss: 1.2056726518494543e-05 \n",
            "Epoch: 496 | Loss: 1.1883716069860384e-05 \n",
            "Epoch: 497 | Loss: 1.171205713035306e-05 \n",
            "Epoch: 498 | Loss: 1.1545200322871096e-05 \n",
            "Epoch: 499 | Loss: 1.1379934221622534e-05 \n",
            "Prediction (after training) 4 8.003877639770508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4XHdeLHlSry"
      },
      "source": [
        "## Pytorch-built Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swKM4ZR8bsyF",
        "outputId": "61f25d59-0421-4215-eb67-750c6d187ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training data and ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data.\n",
        "        \"\"\"\n",
        "        y_pred = sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000 | Loss: 0.5006\n",
            "Epoch 2/1000 | Loss: 0.4998\n",
            "Epoch 3/1000 | Loss: 0.4990\n",
            "Epoch 4/1000 | Loss: 0.4983\n",
            "Epoch 5/1000 | Loss: 0.4976\n",
            "Epoch 6/1000 | Loss: 0.4969\n",
            "Epoch 7/1000 | Loss: 0.4962\n",
            "Epoch 8/1000 | Loss: 0.4955\n",
            "Epoch 9/1000 | Loss: 0.4948\n",
            "Epoch 10/1000 | Loss: 0.4942\n",
            "Epoch 11/1000 | Loss: 0.4935\n",
            "Epoch 12/1000 | Loss: 0.4929\n",
            "Epoch 13/1000 | Loss: 0.4923\n",
            "Epoch 14/1000 | Loss: 0.4917\n",
            "Epoch 15/1000 | Loss: 0.4911\n",
            "Epoch 16/1000 | Loss: 0.4905\n",
            "Epoch 17/1000 | Loss: 0.4899\n",
            "Epoch 18/1000 | Loss: 0.4893\n",
            "Epoch 19/1000 | Loss: 0.4888\n",
            "Epoch 20/1000 | Loss: 0.4883\n",
            "Epoch 21/1000 | Loss: 0.4877\n",
            "Epoch 22/1000 | Loss: 0.4872\n",
            "Epoch 23/1000 | Loss: 0.4867\n",
            "Epoch 24/1000 | Loss: 0.4862\n",
            "Epoch 25/1000 | Loss: 0.4857\n",
            "Epoch 26/1000 | Loss: 0.4852\n",
            "Epoch 27/1000 | Loss: 0.4847\n",
            "Epoch 28/1000 | Loss: 0.4843\n",
            "Epoch 29/1000 | Loss: 0.4838\n",
            "Epoch 30/1000 | Loss: 0.4833\n",
            "Epoch 31/1000 | Loss: 0.4829\n",
            "Epoch 32/1000 | Loss: 0.4825\n",
            "Epoch 33/1000 | Loss: 0.4820\n",
            "Epoch 34/1000 | Loss: 0.4816\n",
            "Epoch 35/1000 | Loss: 0.4812\n",
            "Epoch 36/1000 | Loss: 0.4808\n",
            "Epoch 37/1000 | Loss: 0.4804\n",
            "Epoch 38/1000 | Loss: 0.4800\n",
            "Epoch 39/1000 | Loss: 0.4796\n",
            "Epoch 40/1000 | Loss: 0.4792\n",
            "Epoch 41/1000 | Loss: 0.4789\n",
            "Epoch 42/1000 | Loss: 0.4785\n",
            "Epoch 43/1000 | Loss: 0.4781\n",
            "Epoch 44/1000 | Loss: 0.4778\n",
            "Epoch 45/1000 | Loss: 0.4774\n",
            "Epoch 46/1000 | Loss: 0.4771\n",
            "Epoch 47/1000 | Loss: 0.4767\n",
            "Epoch 48/1000 | Loss: 0.4764\n",
            "Epoch 49/1000 | Loss: 0.4761\n",
            "Epoch 50/1000 | Loss: 0.4758\n",
            "Epoch 51/1000 | Loss: 0.4754\n",
            "Epoch 52/1000 | Loss: 0.4751\n",
            "Epoch 53/1000 | Loss: 0.4748\n",
            "Epoch 54/1000 | Loss: 0.4745\n",
            "Epoch 55/1000 | Loss: 0.4742\n",
            "Epoch 56/1000 | Loss: 0.4739\n",
            "Epoch 57/1000 | Loss: 0.4736\n",
            "Epoch 58/1000 | Loss: 0.4733\n",
            "Epoch 59/1000 | Loss: 0.4730\n",
            "Epoch 60/1000 | Loss: 0.4728\n",
            "Epoch 61/1000 | Loss: 0.4725\n",
            "Epoch 62/1000 | Loss: 0.4722\n",
            "Epoch 63/1000 | Loss: 0.4720\n",
            "Epoch 64/1000 | Loss: 0.4717\n",
            "Epoch 65/1000 | Loss: 0.4714\n",
            "Epoch 66/1000 | Loss: 0.4712\n",
            "Epoch 67/1000 | Loss: 0.4709\n",
            "Epoch 68/1000 | Loss: 0.4707\n",
            "Epoch 69/1000 | Loss: 0.4704\n",
            "Epoch 70/1000 | Loss: 0.4702\n",
            "Epoch 71/1000 | Loss: 0.4699\n",
            "Epoch 72/1000 | Loss: 0.4697\n",
            "Epoch 73/1000 | Loss: 0.4695\n",
            "Epoch 74/1000 | Loss: 0.4692\n",
            "Epoch 75/1000 | Loss: 0.4690\n",
            "Epoch 76/1000 | Loss: 0.4688\n",
            "Epoch 77/1000 | Loss: 0.4685\n",
            "Epoch 78/1000 | Loss: 0.4683\n",
            "Epoch 79/1000 | Loss: 0.4681\n",
            "Epoch 80/1000 | Loss: 0.4679\n",
            "Epoch 81/1000 | Loss: 0.4676\n",
            "Epoch 82/1000 | Loss: 0.4674\n",
            "Epoch 83/1000 | Loss: 0.4672\n",
            "Epoch 84/1000 | Loss: 0.4670\n",
            "Epoch 85/1000 | Loss: 0.4668\n",
            "Epoch 86/1000 | Loss: 0.4666\n",
            "Epoch 87/1000 | Loss: 0.4664\n",
            "Epoch 88/1000 | Loss: 0.4662\n",
            "Epoch 89/1000 | Loss: 0.4660\n",
            "Epoch 90/1000 | Loss: 0.4658\n",
            "Epoch 91/1000 | Loss: 0.4656\n",
            "Epoch 92/1000 | Loss: 0.4654\n",
            "Epoch 93/1000 | Loss: 0.4652\n",
            "Epoch 94/1000 | Loss: 0.4650\n",
            "Epoch 95/1000 | Loss: 0.4648\n",
            "Epoch 96/1000 | Loss: 0.4646\n",
            "Epoch 97/1000 | Loss: 0.4644\n",
            "Epoch 98/1000 | Loss: 0.4642\n",
            "Epoch 99/1000 | Loss: 0.4641\n",
            "Epoch 100/1000 | Loss: 0.4639\n",
            "Epoch 101/1000 | Loss: 0.4637\n",
            "Epoch 102/1000 | Loss: 0.4635\n",
            "Epoch 103/1000 | Loss: 0.4633\n",
            "Epoch 104/1000 | Loss: 0.4632\n",
            "Epoch 105/1000 | Loss: 0.4630\n",
            "Epoch 106/1000 | Loss: 0.4628\n",
            "Epoch 107/1000 | Loss: 0.4626\n",
            "Epoch 108/1000 | Loss: 0.4625\n",
            "Epoch 109/1000 | Loss: 0.4623\n",
            "Epoch 110/1000 | Loss: 0.4621\n",
            "Epoch 111/1000 | Loss: 0.4619\n",
            "Epoch 112/1000 | Loss: 0.4618\n",
            "Epoch 113/1000 | Loss: 0.4616\n",
            "Epoch 114/1000 | Loss: 0.4614\n",
            "Epoch 115/1000 | Loss: 0.4613\n",
            "Epoch 116/1000 | Loss: 0.4611\n",
            "Epoch 117/1000 | Loss: 0.4609\n",
            "Epoch 118/1000 | Loss: 0.4608\n",
            "Epoch 119/1000 | Loss: 0.4606\n",
            "Epoch 120/1000 | Loss: 0.4604\n",
            "Epoch 121/1000 | Loss: 0.4603\n",
            "Epoch 122/1000 | Loss: 0.4601\n",
            "Epoch 123/1000 | Loss: 0.4599\n",
            "Epoch 124/1000 | Loss: 0.4598\n",
            "Epoch 125/1000 | Loss: 0.4596\n",
            "Epoch 126/1000 | Loss: 0.4595\n",
            "Epoch 127/1000 | Loss: 0.4593\n",
            "Epoch 128/1000 | Loss: 0.4591\n",
            "Epoch 129/1000 | Loss: 0.4590\n",
            "Epoch 130/1000 | Loss: 0.4588\n",
            "Epoch 131/1000 | Loss: 0.4587\n",
            "Epoch 132/1000 | Loss: 0.4585\n",
            "Epoch 133/1000 | Loss: 0.4584\n",
            "Epoch 134/1000 | Loss: 0.4582\n",
            "Epoch 135/1000 | Loss: 0.4581\n",
            "Epoch 136/1000 | Loss: 0.4579\n",
            "Epoch 137/1000 | Loss: 0.4578\n",
            "Epoch 138/1000 | Loss: 0.4576\n",
            "Epoch 139/1000 | Loss: 0.4574\n",
            "Epoch 140/1000 | Loss: 0.4573\n",
            "Epoch 141/1000 | Loss: 0.4571\n",
            "Epoch 142/1000 | Loss: 0.4570\n",
            "Epoch 143/1000 | Loss: 0.4568\n",
            "Epoch 144/1000 | Loss: 0.4567\n",
            "Epoch 145/1000 | Loss: 0.4565\n",
            "Epoch 146/1000 | Loss: 0.4564\n",
            "Epoch 147/1000 | Loss: 0.4563\n",
            "Epoch 148/1000 | Loss: 0.4561\n",
            "Epoch 149/1000 | Loss: 0.4560\n",
            "Epoch 150/1000 | Loss: 0.4558\n",
            "Epoch 151/1000 | Loss: 0.4557\n",
            "Epoch 152/1000 | Loss: 0.4555\n",
            "Epoch 153/1000 | Loss: 0.4554\n",
            "Epoch 154/1000 | Loss: 0.4552\n",
            "Epoch 155/1000 | Loss: 0.4551\n",
            "Epoch 156/1000 | Loss: 0.4549\n",
            "Epoch 157/1000 | Loss: 0.4548\n",
            "Epoch 158/1000 | Loss: 0.4546\n",
            "Epoch 159/1000 | Loss: 0.4545\n",
            "Epoch 160/1000 | Loss: 0.4544\n",
            "Epoch 161/1000 | Loss: 0.4542\n",
            "Epoch 162/1000 | Loss: 0.4541\n",
            "Epoch 163/1000 | Loss: 0.4539\n",
            "Epoch 164/1000 | Loss: 0.4538\n",
            "Epoch 165/1000 | Loss: 0.4536\n",
            "Epoch 166/1000 | Loss: 0.4535\n",
            "Epoch 167/1000 | Loss: 0.4534\n",
            "Epoch 168/1000 | Loss: 0.4532\n",
            "Epoch 169/1000 | Loss: 0.4531\n",
            "Epoch 170/1000 | Loss: 0.4529\n",
            "Epoch 171/1000 | Loss: 0.4528\n",
            "Epoch 172/1000 | Loss: 0.4526\n",
            "Epoch 173/1000 | Loss: 0.4525\n",
            "Epoch 174/1000 | Loss: 0.4524\n",
            "Epoch 175/1000 | Loss: 0.4522\n",
            "Epoch 176/1000 | Loss: 0.4521\n",
            "Epoch 177/1000 | Loss: 0.4519\n",
            "Epoch 178/1000 | Loss: 0.4518\n",
            "Epoch 179/1000 | Loss: 0.4517\n",
            "Epoch 180/1000 | Loss: 0.4515\n",
            "Epoch 181/1000 | Loss: 0.4514\n",
            "Epoch 182/1000 | Loss: 0.4513\n",
            "Epoch 183/1000 | Loss: 0.4511\n",
            "Epoch 184/1000 | Loss: 0.4510\n",
            "Epoch 185/1000 | Loss: 0.4508\n",
            "Epoch 186/1000 | Loss: 0.4507\n",
            "Epoch 187/1000 | Loss: 0.4506\n",
            "Epoch 188/1000 | Loss: 0.4504\n",
            "Epoch 189/1000 | Loss: 0.4503\n",
            "Epoch 190/1000 | Loss: 0.4501\n",
            "Epoch 191/1000 | Loss: 0.4500\n",
            "Epoch 192/1000 | Loss: 0.4499\n",
            "Epoch 193/1000 | Loss: 0.4497\n",
            "Epoch 194/1000 | Loss: 0.4496\n",
            "Epoch 195/1000 | Loss: 0.4495\n",
            "Epoch 196/1000 | Loss: 0.4493\n",
            "Epoch 197/1000 | Loss: 0.4492\n",
            "Epoch 198/1000 | Loss: 0.4491\n",
            "Epoch 199/1000 | Loss: 0.4489\n",
            "Epoch 200/1000 | Loss: 0.4488\n",
            "Epoch 201/1000 | Loss: 0.4486\n",
            "Epoch 202/1000 | Loss: 0.4485\n",
            "Epoch 203/1000 | Loss: 0.4484\n",
            "Epoch 204/1000 | Loss: 0.4482\n",
            "Epoch 205/1000 | Loss: 0.4481\n",
            "Epoch 206/1000 | Loss: 0.4480\n",
            "Epoch 207/1000 | Loss: 0.4478\n",
            "Epoch 208/1000 | Loss: 0.4477\n",
            "Epoch 209/1000 | Loss: 0.4476\n",
            "Epoch 210/1000 | Loss: 0.4474\n",
            "Epoch 211/1000 | Loss: 0.4473\n",
            "Epoch 212/1000 | Loss: 0.4472\n",
            "Epoch 213/1000 | Loss: 0.4470\n",
            "Epoch 214/1000 | Loss: 0.4469\n",
            "Epoch 215/1000 | Loss: 0.4468\n",
            "Epoch 216/1000 | Loss: 0.4466\n",
            "Epoch 217/1000 | Loss: 0.4465\n",
            "Epoch 218/1000 | Loss: 0.4464\n",
            "Epoch 219/1000 | Loss: 0.4462\n",
            "Epoch 220/1000 | Loss: 0.4461\n",
            "Epoch 221/1000 | Loss: 0.4460\n",
            "Epoch 222/1000 | Loss: 0.4458\n",
            "Epoch 223/1000 | Loss: 0.4457\n",
            "Epoch 224/1000 | Loss: 0.4456\n",
            "Epoch 225/1000 | Loss: 0.4454\n",
            "Epoch 226/1000 | Loss: 0.4453\n",
            "Epoch 227/1000 | Loss: 0.4452\n",
            "Epoch 228/1000 | Loss: 0.4450\n",
            "Epoch 229/1000 | Loss: 0.4449\n",
            "Epoch 230/1000 | Loss: 0.4448\n",
            "Epoch 231/1000 | Loss: 0.4446\n",
            "Epoch 232/1000 | Loss: 0.4445\n",
            "Epoch 233/1000 | Loss: 0.4444\n",
            "Epoch 234/1000 | Loss: 0.4442\n",
            "Epoch 235/1000 | Loss: 0.4441\n",
            "Epoch 236/1000 | Loss: 0.4440\n",
            "Epoch 237/1000 | Loss: 0.4439\n",
            "Epoch 238/1000 | Loss: 0.4437\n",
            "Epoch 239/1000 | Loss: 0.4436\n",
            "Epoch 240/1000 | Loss: 0.4435\n",
            "Epoch 241/1000 | Loss: 0.4433\n",
            "Epoch 242/1000 | Loss: 0.4432\n",
            "Epoch 243/1000 | Loss: 0.4431\n",
            "Epoch 244/1000 | Loss: 0.4429\n",
            "Epoch 245/1000 | Loss: 0.4428\n",
            "Epoch 246/1000 | Loss: 0.4427\n",
            "Epoch 247/1000 | Loss: 0.4425\n",
            "Epoch 248/1000 | Loss: 0.4424\n",
            "Epoch 249/1000 | Loss: 0.4423\n",
            "Epoch 250/1000 | Loss: 0.4422\n",
            "Epoch 251/1000 | Loss: 0.4420\n",
            "Epoch 252/1000 | Loss: 0.4419\n",
            "Epoch 253/1000 | Loss: 0.4418\n",
            "Epoch 254/1000 | Loss: 0.4416\n",
            "Epoch 255/1000 | Loss: 0.4415\n",
            "Epoch 256/1000 | Loss: 0.4414\n",
            "Epoch 257/1000 | Loss: 0.4412\n",
            "Epoch 258/1000 | Loss: 0.4411\n",
            "Epoch 259/1000 | Loss: 0.4410\n",
            "Epoch 260/1000 | Loss: 0.4409\n",
            "Epoch 261/1000 | Loss: 0.4407\n",
            "Epoch 262/1000 | Loss: 0.4406\n",
            "Epoch 263/1000 | Loss: 0.4405\n",
            "Epoch 264/1000 | Loss: 0.4403\n",
            "Epoch 265/1000 | Loss: 0.4402\n",
            "Epoch 266/1000 | Loss: 0.4401\n",
            "Epoch 267/1000 | Loss: 0.4400\n",
            "Epoch 268/1000 | Loss: 0.4398\n",
            "Epoch 269/1000 | Loss: 0.4397\n",
            "Epoch 270/1000 | Loss: 0.4396\n",
            "Epoch 271/1000 | Loss: 0.4394\n",
            "Epoch 272/1000 | Loss: 0.4393\n",
            "Epoch 273/1000 | Loss: 0.4392\n",
            "Epoch 274/1000 | Loss: 0.4391\n",
            "Epoch 275/1000 | Loss: 0.4389\n",
            "Epoch 276/1000 | Loss: 0.4388\n",
            "Epoch 277/1000 | Loss: 0.4387\n",
            "Epoch 278/1000 | Loss: 0.4386\n",
            "Epoch 279/1000 | Loss: 0.4384\n",
            "Epoch 280/1000 | Loss: 0.4383\n",
            "Epoch 281/1000 | Loss: 0.4382\n",
            "Epoch 282/1000 | Loss: 0.4380\n",
            "Epoch 283/1000 | Loss: 0.4379\n",
            "Epoch 284/1000 | Loss: 0.4378\n",
            "Epoch 285/1000 | Loss: 0.4377\n",
            "Epoch 286/1000 | Loss: 0.4375\n",
            "Epoch 287/1000 | Loss: 0.4374\n",
            "Epoch 288/1000 | Loss: 0.4373\n",
            "Epoch 289/1000 | Loss: 0.4372\n",
            "Epoch 290/1000 | Loss: 0.4370\n",
            "Epoch 291/1000 | Loss: 0.4369\n",
            "Epoch 292/1000 | Loss: 0.4368\n",
            "Epoch 293/1000 | Loss: 0.4366\n",
            "Epoch 294/1000 | Loss: 0.4365\n",
            "Epoch 295/1000 | Loss: 0.4364\n",
            "Epoch 296/1000 | Loss: 0.4363\n",
            "Epoch 297/1000 | Loss: 0.4361\n",
            "Epoch 298/1000 | Loss: 0.4360\n",
            "Epoch 299/1000 | Loss: 0.4359\n",
            "Epoch 300/1000 | Loss: 0.4358\n",
            "Epoch 301/1000 | Loss: 0.4356\n",
            "Epoch 302/1000 | Loss: 0.4355\n",
            "Epoch 303/1000 | Loss: 0.4354\n",
            "Epoch 304/1000 | Loss: 0.4353\n",
            "Epoch 305/1000 | Loss: 0.4351\n",
            "Epoch 306/1000 | Loss: 0.4350\n",
            "Epoch 307/1000 | Loss: 0.4349\n",
            "Epoch 308/1000 | Loss: 0.4348\n",
            "Epoch 309/1000 | Loss: 0.4346\n",
            "Epoch 310/1000 | Loss: 0.4345\n",
            "Epoch 311/1000 | Loss: 0.4344\n",
            "Epoch 312/1000 | Loss: 0.4343\n",
            "Epoch 313/1000 | Loss: 0.4341\n",
            "Epoch 314/1000 | Loss: 0.4340\n",
            "Epoch 315/1000 | Loss: 0.4339\n",
            "Epoch 316/1000 | Loss: 0.4338\n",
            "Epoch 317/1000 | Loss: 0.4336\n",
            "Epoch 318/1000 | Loss: 0.4335\n",
            "Epoch 319/1000 | Loss: 0.4334\n",
            "Epoch 320/1000 | Loss: 0.4333\n",
            "Epoch 321/1000 | Loss: 0.4331\n",
            "Epoch 322/1000 | Loss: 0.4330\n",
            "Epoch 323/1000 | Loss: 0.4329\n",
            "Epoch 324/1000 | Loss: 0.4328\n",
            "Epoch 325/1000 | Loss: 0.4327\n",
            "Epoch 326/1000 | Loss: 0.4325\n",
            "Epoch 327/1000 | Loss: 0.4324\n",
            "Epoch 328/1000 | Loss: 0.4323\n",
            "Epoch 329/1000 | Loss: 0.4322\n",
            "Epoch 330/1000 | Loss: 0.4320\n",
            "Epoch 331/1000 | Loss: 0.4319\n",
            "Epoch 332/1000 | Loss: 0.4318\n",
            "Epoch 333/1000 | Loss: 0.4317\n",
            "Epoch 334/1000 | Loss: 0.4315\n",
            "Epoch 335/1000 | Loss: 0.4314\n",
            "Epoch 336/1000 | Loss: 0.4313\n",
            "Epoch 337/1000 | Loss: 0.4312\n",
            "Epoch 338/1000 | Loss: 0.4311\n",
            "Epoch 339/1000 | Loss: 0.4309\n",
            "Epoch 340/1000 | Loss: 0.4308\n",
            "Epoch 341/1000 | Loss: 0.4307\n",
            "Epoch 342/1000 | Loss: 0.4306\n",
            "Epoch 343/1000 | Loss: 0.4304\n",
            "Epoch 344/1000 | Loss: 0.4303\n",
            "Epoch 345/1000 | Loss: 0.4302\n",
            "Epoch 346/1000 | Loss: 0.4301\n",
            "Epoch 347/1000 | Loss: 0.4300\n",
            "Epoch 348/1000 | Loss: 0.4298\n",
            "Epoch 349/1000 | Loss: 0.4297\n",
            "Epoch 350/1000 | Loss: 0.4296\n",
            "Epoch 351/1000 | Loss: 0.4295\n",
            "Epoch 352/1000 | Loss: 0.4293\n",
            "Epoch 353/1000 | Loss: 0.4292\n",
            "Epoch 354/1000 | Loss: 0.4291\n",
            "Epoch 355/1000 | Loss: 0.4290\n",
            "Epoch 356/1000 | Loss: 0.4289\n",
            "Epoch 357/1000 | Loss: 0.4287\n",
            "Epoch 358/1000 | Loss: 0.4286\n",
            "Epoch 359/1000 | Loss: 0.4285\n",
            "Epoch 360/1000 | Loss: 0.4284\n",
            "Epoch 361/1000 | Loss: 0.4283\n",
            "Epoch 362/1000 | Loss: 0.4281\n",
            "Epoch 363/1000 | Loss: 0.4280\n",
            "Epoch 364/1000 | Loss: 0.4279\n",
            "Epoch 365/1000 | Loss: 0.4278\n",
            "Epoch 366/1000 | Loss: 0.4277\n",
            "Epoch 367/1000 | Loss: 0.4275\n",
            "Epoch 368/1000 | Loss: 0.4274\n",
            "Epoch 369/1000 | Loss: 0.4273\n",
            "Epoch 370/1000 | Loss: 0.4272\n",
            "Epoch 371/1000 | Loss: 0.4271\n",
            "Epoch 372/1000 | Loss: 0.4269\n",
            "Epoch 373/1000 | Loss: 0.4268\n",
            "Epoch 374/1000 | Loss: 0.4267\n",
            "Epoch 375/1000 | Loss: 0.4266\n",
            "Epoch 376/1000 | Loss: 0.4265\n",
            "Epoch 377/1000 | Loss: 0.4263\n",
            "Epoch 378/1000 | Loss: 0.4262\n",
            "Epoch 379/1000 | Loss: 0.4261\n",
            "Epoch 380/1000 | Loss: 0.4260\n",
            "Epoch 381/1000 | Loss: 0.4259\n",
            "Epoch 382/1000 | Loss: 0.4257\n",
            "Epoch 383/1000 | Loss: 0.4256\n",
            "Epoch 384/1000 | Loss: 0.4255\n",
            "Epoch 385/1000 | Loss: 0.4254\n",
            "Epoch 386/1000 | Loss: 0.4253\n",
            "Epoch 387/1000 | Loss: 0.4251\n",
            "Epoch 388/1000 | Loss: 0.4250\n",
            "Epoch 389/1000 | Loss: 0.4249\n",
            "Epoch 390/1000 | Loss: 0.4248\n",
            "Epoch 391/1000 | Loss: 0.4247\n",
            "Epoch 392/1000 | Loss: 0.4245\n",
            "Epoch 393/1000 | Loss: 0.4244\n",
            "Epoch 394/1000 | Loss: 0.4243\n",
            "Epoch 395/1000 | Loss: 0.4242\n",
            "Epoch 396/1000 | Loss: 0.4241\n",
            "Epoch 397/1000 | Loss: 0.4240\n",
            "Epoch 398/1000 | Loss: 0.4238\n",
            "Epoch 399/1000 | Loss: 0.4237\n",
            "Epoch 400/1000 | Loss: 0.4236\n",
            "Epoch 401/1000 | Loss: 0.4235\n",
            "Epoch 402/1000 | Loss: 0.4234\n",
            "Epoch 403/1000 | Loss: 0.4232\n",
            "Epoch 404/1000 | Loss: 0.4231\n",
            "Epoch 405/1000 | Loss: 0.4230\n",
            "Epoch 406/1000 | Loss: 0.4229\n",
            "Epoch 407/1000 | Loss: 0.4228\n",
            "Epoch 408/1000 | Loss: 0.4227\n",
            "Epoch 409/1000 | Loss: 0.4225\n",
            "Epoch 410/1000 | Loss: 0.4224\n",
            "Epoch 411/1000 | Loss: 0.4223\n",
            "Epoch 412/1000 | Loss: 0.4222\n",
            "Epoch 413/1000 | Loss: 0.4221\n",
            "Epoch 414/1000 | Loss: 0.4220\n",
            "Epoch 415/1000 | Loss: 0.4218\n",
            "Epoch 416/1000 | Loss: 0.4217\n",
            "Epoch 417/1000 | Loss: 0.4216\n",
            "Epoch 418/1000 | Loss: 0.4215\n",
            "Epoch 419/1000 | Loss: 0.4214\n",
            "Epoch 420/1000 | Loss: 0.4213\n",
            "Epoch 421/1000 | Loss: 0.4211\n",
            "Epoch 422/1000 | Loss: 0.4210\n",
            "Epoch 423/1000 | Loss: 0.4209\n",
            "Epoch 424/1000 | Loss: 0.4208\n",
            "Epoch 425/1000 | Loss: 0.4207\n",
            "Epoch 426/1000 | Loss: 0.4206\n",
            "Epoch 427/1000 | Loss: 0.4204\n",
            "Epoch 428/1000 | Loss: 0.4203\n",
            "Epoch 429/1000 | Loss: 0.4202\n",
            "Epoch 430/1000 | Loss: 0.4201\n",
            "Epoch 431/1000 | Loss: 0.4200\n",
            "Epoch 432/1000 | Loss: 0.4199\n",
            "Epoch 433/1000 | Loss: 0.4197\n",
            "Epoch 434/1000 | Loss: 0.4196\n",
            "Epoch 435/1000 | Loss: 0.4195\n",
            "Epoch 436/1000 | Loss: 0.4194\n",
            "Epoch 437/1000 | Loss: 0.4193\n",
            "Epoch 438/1000 | Loss: 0.4192\n",
            "Epoch 439/1000 | Loss: 0.4191\n",
            "Epoch 440/1000 | Loss: 0.4189\n",
            "Epoch 441/1000 | Loss: 0.4188\n",
            "Epoch 442/1000 | Loss: 0.4187\n",
            "Epoch 443/1000 | Loss: 0.4186\n",
            "Epoch 444/1000 | Loss: 0.4185\n",
            "Epoch 445/1000 | Loss: 0.4184\n",
            "Epoch 446/1000 | Loss: 0.4183\n",
            "Epoch 447/1000 | Loss: 0.4181\n",
            "Epoch 448/1000 | Loss: 0.4180\n",
            "Epoch 449/1000 | Loss: 0.4179\n",
            "Epoch 450/1000 | Loss: 0.4178\n",
            "Epoch 451/1000 | Loss: 0.4177\n",
            "Epoch 452/1000 | Loss: 0.4176\n",
            "Epoch 453/1000 | Loss: 0.4175\n",
            "Epoch 454/1000 | Loss: 0.4173\n",
            "Epoch 455/1000 | Loss: 0.4172\n",
            "Epoch 456/1000 | Loss: 0.4171\n",
            "Epoch 457/1000 | Loss: 0.4170\n",
            "Epoch 458/1000 | Loss: 0.4169\n",
            "Epoch 459/1000 | Loss: 0.4168\n",
            "Epoch 460/1000 | Loss: 0.4167\n",
            "Epoch 461/1000 | Loss: 0.4165\n",
            "Epoch 462/1000 | Loss: 0.4164\n",
            "Epoch 463/1000 | Loss: 0.4163\n",
            "Epoch 464/1000 | Loss: 0.4162\n",
            "Epoch 465/1000 | Loss: 0.4161\n",
            "Epoch 466/1000 | Loss: 0.4160\n",
            "Epoch 467/1000 | Loss: 0.4159\n",
            "Epoch 468/1000 | Loss: 0.4157\n",
            "Epoch 469/1000 | Loss: 0.4156\n",
            "Epoch 470/1000 | Loss: 0.4155\n",
            "Epoch 471/1000 | Loss: 0.4154\n",
            "Epoch 472/1000 | Loss: 0.4153\n",
            "Epoch 473/1000 | Loss: 0.4152\n",
            "Epoch 474/1000 | Loss: 0.4151\n",
            "Epoch 475/1000 | Loss: 0.4150\n",
            "Epoch 476/1000 | Loss: 0.4148\n",
            "Epoch 477/1000 | Loss: 0.4147\n",
            "Epoch 478/1000 | Loss: 0.4146\n",
            "Epoch 479/1000 | Loss: 0.4145\n",
            "Epoch 480/1000 | Loss: 0.4144\n",
            "Epoch 481/1000 | Loss: 0.4143\n",
            "Epoch 482/1000 | Loss: 0.4142\n",
            "Epoch 483/1000 | Loss: 0.4141\n",
            "Epoch 484/1000 | Loss: 0.4139\n",
            "Epoch 485/1000 | Loss: 0.4138\n",
            "Epoch 486/1000 | Loss: 0.4137\n",
            "Epoch 487/1000 | Loss: 0.4136\n",
            "Epoch 488/1000 | Loss: 0.4135\n",
            "Epoch 489/1000 | Loss: 0.4134\n",
            "Epoch 490/1000 | Loss: 0.4133\n",
            "Epoch 491/1000 | Loss: 0.4132\n",
            "Epoch 492/1000 | Loss: 0.4131\n",
            "Epoch 493/1000 | Loss: 0.4129\n",
            "Epoch 494/1000 | Loss: 0.4128\n",
            "Epoch 495/1000 | Loss: 0.4127\n",
            "Epoch 496/1000 | Loss: 0.4126\n",
            "Epoch 497/1000 | Loss: 0.4125\n",
            "Epoch 498/1000 | Loss: 0.4124\n",
            "Epoch 499/1000 | Loss: 0.4123\n",
            "Epoch 500/1000 | Loss: 0.4122\n",
            "Epoch 501/1000 | Loss: 0.4121\n",
            "Epoch 502/1000 | Loss: 0.4119\n",
            "Epoch 503/1000 | Loss: 0.4118\n",
            "Epoch 504/1000 | Loss: 0.4117\n",
            "Epoch 505/1000 | Loss: 0.4116\n",
            "Epoch 506/1000 | Loss: 0.4115\n",
            "Epoch 507/1000 | Loss: 0.4114\n",
            "Epoch 508/1000 | Loss: 0.4113\n",
            "Epoch 509/1000 | Loss: 0.4112\n",
            "Epoch 510/1000 | Loss: 0.4111\n",
            "Epoch 511/1000 | Loss: 0.4109\n",
            "Epoch 512/1000 | Loss: 0.4108\n",
            "Epoch 513/1000 | Loss: 0.4107\n",
            "Epoch 514/1000 | Loss: 0.4106\n",
            "Epoch 515/1000 | Loss: 0.4105\n",
            "Epoch 516/1000 | Loss: 0.4104\n",
            "Epoch 517/1000 | Loss: 0.4103\n",
            "Epoch 518/1000 | Loss: 0.4102\n",
            "Epoch 519/1000 | Loss: 0.4101\n",
            "Epoch 520/1000 | Loss: 0.4100\n",
            "Epoch 521/1000 | Loss: 0.4098\n",
            "Epoch 522/1000 | Loss: 0.4097\n",
            "Epoch 523/1000 | Loss: 0.4096\n",
            "Epoch 524/1000 | Loss: 0.4095\n",
            "Epoch 525/1000 | Loss: 0.4094\n",
            "Epoch 526/1000 | Loss: 0.4093\n",
            "Epoch 527/1000 | Loss: 0.4092\n",
            "Epoch 528/1000 | Loss: 0.4091\n",
            "Epoch 529/1000 | Loss: 0.4090\n",
            "Epoch 530/1000 | Loss: 0.4089\n",
            "Epoch 531/1000 | Loss: 0.4088\n",
            "Epoch 532/1000 | Loss: 0.4086\n",
            "Epoch 533/1000 | Loss: 0.4085\n",
            "Epoch 534/1000 | Loss: 0.4084\n",
            "Epoch 535/1000 | Loss: 0.4083\n",
            "Epoch 536/1000 | Loss: 0.4082\n",
            "Epoch 537/1000 | Loss: 0.4081\n",
            "Epoch 538/1000 | Loss: 0.4080\n",
            "Epoch 539/1000 | Loss: 0.4079\n",
            "Epoch 540/1000 | Loss: 0.4078\n",
            "Epoch 541/1000 | Loss: 0.4077\n",
            "Epoch 542/1000 | Loss: 0.4076\n",
            "Epoch 543/1000 | Loss: 0.4075\n",
            "Epoch 544/1000 | Loss: 0.4073\n",
            "Epoch 545/1000 | Loss: 0.4072\n",
            "Epoch 546/1000 | Loss: 0.4071\n",
            "Epoch 547/1000 | Loss: 0.4070\n",
            "Epoch 548/1000 | Loss: 0.4069\n",
            "Epoch 549/1000 | Loss: 0.4068\n",
            "Epoch 550/1000 | Loss: 0.4067\n",
            "Epoch 551/1000 | Loss: 0.4066\n",
            "Epoch 552/1000 | Loss: 0.4065\n",
            "Epoch 553/1000 | Loss: 0.4064\n",
            "Epoch 554/1000 | Loss: 0.4063\n",
            "Epoch 555/1000 | Loss: 0.4062\n",
            "Epoch 556/1000 | Loss: 0.4061\n",
            "Epoch 557/1000 | Loss: 0.4059\n",
            "Epoch 558/1000 | Loss: 0.4058\n",
            "Epoch 559/1000 | Loss: 0.4057\n",
            "Epoch 560/1000 | Loss: 0.4056\n",
            "Epoch 561/1000 | Loss: 0.4055\n",
            "Epoch 562/1000 | Loss: 0.4054\n",
            "Epoch 563/1000 | Loss: 0.4053\n",
            "Epoch 564/1000 | Loss: 0.4052\n",
            "Epoch 565/1000 | Loss: 0.4051\n",
            "Epoch 566/1000 | Loss: 0.4050\n",
            "Epoch 567/1000 | Loss: 0.4049\n",
            "Epoch 568/1000 | Loss: 0.4048\n",
            "Epoch 569/1000 | Loss: 0.4047\n",
            "Epoch 570/1000 | Loss: 0.4046\n",
            "Epoch 571/1000 | Loss: 0.4045\n",
            "Epoch 572/1000 | Loss: 0.4043\n",
            "Epoch 573/1000 | Loss: 0.4042\n",
            "Epoch 574/1000 | Loss: 0.4041\n",
            "Epoch 575/1000 | Loss: 0.4040\n",
            "Epoch 576/1000 | Loss: 0.4039\n",
            "Epoch 577/1000 | Loss: 0.4038\n",
            "Epoch 578/1000 | Loss: 0.4037\n",
            "Epoch 579/1000 | Loss: 0.4036\n",
            "Epoch 580/1000 | Loss: 0.4035\n",
            "Epoch 581/1000 | Loss: 0.4034\n",
            "Epoch 582/1000 | Loss: 0.4033\n",
            "Epoch 583/1000 | Loss: 0.4032\n",
            "Epoch 584/1000 | Loss: 0.4031\n",
            "Epoch 585/1000 | Loss: 0.4030\n",
            "Epoch 586/1000 | Loss: 0.4029\n",
            "Epoch 587/1000 | Loss: 0.4028\n",
            "Epoch 588/1000 | Loss: 0.4027\n",
            "Epoch 589/1000 | Loss: 0.4025\n",
            "Epoch 590/1000 | Loss: 0.4024\n",
            "Epoch 591/1000 | Loss: 0.4023\n",
            "Epoch 592/1000 | Loss: 0.4022\n",
            "Epoch 593/1000 | Loss: 0.4021\n",
            "Epoch 594/1000 | Loss: 0.4020\n",
            "Epoch 595/1000 | Loss: 0.4019\n",
            "Epoch 596/1000 | Loss: 0.4018\n",
            "Epoch 597/1000 | Loss: 0.4017\n",
            "Epoch 598/1000 | Loss: 0.4016\n",
            "Epoch 599/1000 | Loss: 0.4015\n",
            "Epoch 600/1000 | Loss: 0.4014\n",
            "Epoch 601/1000 | Loss: 0.4013\n",
            "Epoch 602/1000 | Loss: 0.4012\n",
            "Epoch 603/1000 | Loss: 0.4011\n",
            "Epoch 604/1000 | Loss: 0.4010\n",
            "Epoch 605/1000 | Loss: 0.4009\n",
            "Epoch 606/1000 | Loss: 0.4008\n",
            "Epoch 607/1000 | Loss: 0.4007\n",
            "Epoch 608/1000 | Loss: 0.4006\n",
            "Epoch 609/1000 | Loss: 0.4005\n",
            "Epoch 610/1000 | Loss: 0.4004\n",
            "Epoch 611/1000 | Loss: 0.4002\n",
            "Epoch 612/1000 | Loss: 0.4001\n",
            "Epoch 613/1000 | Loss: 0.4000\n",
            "Epoch 614/1000 | Loss: 0.3999\n",
            "Epoch 615/1000 | Loss: 0.3998\n",
            "Epoch 616/1000 | Loss: 0.3997\n",
            "Epoch 617/1000 | Loss: 0.3996\n",
            "Epoch 618/1000 | Loss: 0.3995\n",
            "Epoch 619/1000 | Loss: 0.3994\n",
            "Epoch 620/1000 | Loss: 0.3993\n",
            "Epoch 621/1000 | Loss: 0.3992\n",
            "Epoch 622/1000 | Loss: 0.3991\n",
            "Epoch 623/1000 | Loss: 0.3990\n",
            "Epoch 624/1000 | Loss: 0.3989\n",
            "Epoch 625/1000 | Loss: 0.3988\n",
            "Epoch 626/1000 | Loss: 0.3987\n",
            "Epoch 627/1000 | Loss: 0.3986\n",
            "Epoch 628/1000 | Loss: 0.3985\n",
            "Epoch 629/1000 | Loss: 0.3984\n",
            "Epoch 630/1000 | Loss: 0.3983\n",
            "Epoch 631/1000 | Loss: 0.3982\n",
            "Epoch 632/1000 | Loss: 0.3981\n",
            "Epoch 633/1000 | Loss: 0.3980\n",
            "Epoch 634/1000 | Loss: 0.3979\n",
            "Epoch 635/1000 | Loss: 0.3978\n",
            "Epoch 636/1000 | Loss: 0.3977\n",
            "Epoch 637/1000 | Loss: 0.3976\n",
            "Epoch 638/1000 | Loss: 0.3975\n",
            "Epoch 639/1000 | Loss: 0.3974\n",
            "Epoch 640/1000 | Loss: 0.3973\n",
            "Epoch 641/1000 | Loss: 0.3972\n",
            "Epoch 642/1000 | Loss: 0.3971\n",
            "Epoch 643/1000 | Loss: 0.3970\n",
            "Epoch 644/1000 | Loss: 0.3969\n",
            "Epoch 645/1000 | Loss: 0.3968\n",
            "Epoch 646/1000 | Loss: 0.3967\n",
            "Epoch 647/1000 | Loss: 0.3965\n",
            "Epoch 648/1000 | Loss: 0.3964\n",
            "Epoch 649/1000 | Loss: 0.3963\n",
            "Epoch 650/1000 | Loss: 0.3962\n",
            "Epoch 651/1000 | Loss: 0.3961\n",
            "Epoch 652/1000 | Loss: 0.3960\n",
            "Epoch 653/1000 | Loss: 0.3959\n",
            "Epoch 654/1000 | Loss: 0.3958\n",
            "Epoch 655/1000 | Loss: 0.3957\n",
            "Epoch 656/1000 | Loss: 0.3956\n",
            "Epoch 657/1000 | Loss: 0.3955\n",
            "Epoch 658/1000 | Loss: 0.3954\n",
            "Epoch 659/1000 | Loss: 0.3953\n",
            "Epoch 660/1000 | Loss: 0.3952\n",
            "Epoch 661/1000 | Loss: 0.3951\n",
            "Epoch 662/1000 | Loss: 0.3950\n",
            "Epoch 663/1000 | Loss: 0.3949\n",
            "Epoch 664/1000 | Loss: 0.3948\n",
            "Epoch 665/1000 | Loss: 0.3947\n",
            "Epoch 666/1000 | Loss: 0.3946\n",
            "Epoch 667/1000 | Loss: 0.3945\n",
            "Epoch 668/1000 | Loss: 0.3944\n",
            "Epoch 669/1000 | Loss: 0.3943\n",
            "Epoch 670/1000 | Loss: 0.3942\n",
            "Epoch 671/1000 | Loss: 0.3941\n",
            "Epoch 672/1000 | Loss: 0.3940\n",
            "Epoch 673/1000 | Loss: 0.3939\n",
            "Epoch 674/1000 | Loss: 0.3938\n",
            "Epoch 675/1000 | Loss: 0.3937\n",
            "Epoch 676/1000 | Loss: 0.3936\n",
            "Epoch 677/1000 | Loss: 0.3935\n",
            "Epoch 678/1000 | Loss: 0.3934\n",
            "Epoch 679/1000 | Loss: 0.3933\n",
            "Epoch 680/1000 | Loss: 0.3932\n",
            "Epoch 681/1000 | Loss: 0.3931\n",
            "Epoch 682/1000 | Loss: 0.3930\n",
            "Epoch 683/1000 | Loss: 0.3929\n",
            "Epoch 684/1000 | Loss: 0.3928\n",
            "Epoch 685/1000 | Loss: 0.3927\n",
            "Epoch 686/1000 | Loss: 0.3926\n",
            "Epoch 687/1000 | Loss: 0.3925\n",
            "Epoch 688/1000 | Loss: 0.3924\n",
            "Epoch 689/1000 | Loss: 0.3923\n",
            "Epoch 690/1000 | Loss: 0.3922\n",
            "Epoch 691/1000 | Loss: 0.3921\n",
            "Epoch 692/1000 | Loss: 0.3920\n",
            "Epoch 693/1000 | Loss: 0.3919\n",
            "Epoch 694/1000 | Loss: 0.3918\n",
            "Epoch 695/1000 | Loss: 0.3917\n",
            "Epoch 696/1000 | Loss: 0.3916\n",
            "Epoch 697/1000 | Loss: 0.3915\n",
            "Epoch 698/1000 | Loss: 0.3914\n",
            "Epoch 699/1000 | Loss: 0.3913\n",
            "Epoch 700/1000 | Loss: 0.3912\n",
            "Epoch 701/1000 | Loss: 0.3911\n",
            "Epoch 702/1000 | Loss: 0.3910\n",
            "Epoch 703/1000 | Loss: 0.3909\n",
            "Epoch 704/1000 | Loss: 0.3909\n",
            "Epoch 705/1000 | Loss: 0.3908\n",
            "Epoch 706/1000 | Loss: 0.3907\n",
            "Epoch 707/1000 | Loss: 0.3906\n",
            "Epoch 708/1000 | Loss: 0.3905\n",
            "Epoch 709/1000 | Loss: 0.3904\n",
            "Epoch 710/1000 | Loss: 0.3903\n",
            "Epoch 711/1000 | Loss: 0.3902\n",
            "Epoch 712/1000 | Loss: 0.3901\n",
            "Epoch 713/1000 | Loss: 0.3900\n",
            "Epoch 714/1000 | Loss: 0.3899\n",
            "Epoch 715/1000 | Loss: 0.3898\n",
            "Epoch 716/1000 | Loss: 0.3897\n",
            "Epoch 717/1000 | Loss: 0.3896\n",
            "Epoch 718/1000 | Loss: 0.3895\n",
            "Epoch 719/1000 | Loss: 0.3894\n",
            "Epoch 720/1000 | Loss: 0.3893\n",
            "Epoch 721/1000 | Loss: 0.3892\n",
            "Epoch 722/1000 | Loss: 0.3891\n",
            "Epoch 723/1000 | Loss: 0.3890\n",
            "Epoch 724/1000 | Loss: 0.3889\n",
            "Epoch 725/1000 | Loss: 0.3888\n",
            "Epoch 726/1000 | Loss: 0.3887\n",
            "Epoch 727/1000 | Loss: 0.3886\n",
            "Epoch 728/1000 | Loss: 0.3885\n",
            "Epoch 729/1000 | Loss: 0.3884\n",
            "Epoch 730/1000 | Loss: 0.3883\n",
            "Epoch 731/1000 | Loss: 0.3882\n",
            "Epoch 732/1000 | Loss: 0.3881\n",
            "Epoch 733/1000 | Loss: 0.3880\n",
            "Epoch 734/1000 | Loss: 0.3879\n",
            "Epoch 735/1000 | Loss: 0.3878\n",
            "Epoch 736/1000 | Loss: 0.3877\n",
            "Epoch 737/1000 | Loss: 0.3876\n",
            "Epoch 738/1000 | Loss: 0.3875\n",
            "Epoch 739/1000 | Loss: 0.3874\n",
            "Epoch 740/1000 | Loss: 0.3873\n",
            "Epoch 741/1000 | Loss: 0.3873\n",
            "Epoch 742/1000 | Loss: 0.3872\n",
            "Epoch 743/1000 | Loss: 0.3871\n",
            "Epoch 744/1000 | Loss: 0.3870\n",
            "Epoch 745/1000 | Loss: 0.3869\n",
            "Epoch 746/1000 | Loss: 0.3868\n",
            "Epoch 747/1000 | Loss: 0.3867\n",
            "Epoch 748/1000 | Loss: 0.3866\n",
            "Epoch 749/1000 | Loss: 0.3865\n",
            "Epoch 750/1000 | Loss: 0.3864\n",
            "Epoch 751/1000 | Loss: 0.3863\n",
            "Epoch 752/1000 | Loss: 0.3862\n",
            "Epoch 753/1000 | Loss: 0.3861\n",
            "Epoch 754/1000 | Loss: 0.3860\n",
            "Epoch 755/1000 | Loss: 0.3859\n",
            "Epoch 756/1000 | Loss: 0.3858\n",
            "Epoch 757/1000 | Loss: 0.3857\n",
            "Epoch 758/1000 | Loss: 0.3856\n",
            "Epoch 759/1000 | Loss: 0.3855\n",
            "Epoch 760/1000 | Loss: 0.3854\n",
            "Epoch 761/1000 | Loss: 0.3853\n",
            "Epoch 762/1000 | Loss: 0.3852\n",
            "Epoch 763/1000 | Loss: 0.3851\n",
            "Epoch 764/1000 | Loss: 0.3851\n",
            "Epoch 765/1000 | Loss: 0.3850\n",
            "Epoch 766/1000 | Loss: 0.3849\n",
            "Epoch 767/1000 | Loss: 0.3848\n",
            "Epoch 768/1000 | Loss: 0.3847\n",
            "Epoch 769/1000 | Loss: 0.3846\n",
            "Epoch 770/1000 | Loss: 0.3845\n",
            "Epoch 771/1000 | Loss: 0.3844\n",
            "Epoch 772/1000 | Loss: 0.3843\n",
            "Epoch 773/1000 | Loss: 0.3842\n",
            "Epoch 774/1000 | Loss: 0.3841\n",
            "Epoch 775/1000 | Loss: 0.3840\n",
            "Epoch 776/1000 | Loss: 0.3839\n",
            "Epoch 777/1000 | Loss: 0.3838\n",
            "Epoch 778/1000 | Loss: 0.3837\n",
            "Epoch 779/1000 | Loss: 0.3836\n",
            "Epoch 780/1000 | Loss: 0.3835\n",
            "Epoch 781/1000 | Loss: 0.3834\n",
            "Epoch 782/1000 | Loss: 0.3834\n",
            "Epoch 783/1000 | Loss: 0.3833\n",
            "Epoch 784/1000 | Loss: 0.3832\n",
            "Epoch 785/1000 | Loss: 0.3831\n",
            "Epoch 786/1000 | Loss: 0.3830\n",
            "Epoch 787/1000 | Loss: 0.3829\n",
            "Epoch 788/1000 | Loss: 0.3828\n",
            "Epoch 789/1000 | Loss: 0.3827\n",
            "Epoch 790/1000 | Loss: 0.3826\n",
            "Epoch 791/1000 | Loss: 0.3825\n",
            "Epoch 792/1000 | Loss: 0.3824\n",
            "Epoch 793/1000 | Loss: 0.3823\n",
            "Epoch 794/1000 | Loss: 0.3822\n",
            "Epoch 795/1000 | Loss: 0.3821\n",
            "Epoch 796/1000 | Loss: 0.3820\n",
            "Epoch 797/1000 | Loss: 0.3819\n",
            "Epoch 798/1000 | Loss: 0.3819\n",
            "Epoch 799/1000 | Loss: 0.3818\n",
            "Epoch 800/1000 | Loss: 0.3817\n",
            "Epoch 801/1000 | Loss: 0.3816\n",
            "Epoch 802/1000 | Loss: 0.3815\n",
            "Epoch 803/1000 | Loss: 0.3814\n",
            "Epoch 804/1000 | Loss: 0.3813\n",
            "Epoch 805/1000 | Loss: 0.3812\n",
            "Epoch 806/1000 | Loss: 0.3811\n",
            "Epoch 807/1000 | Loss: 0.3810\n",
            "Epoch 808/1000 | Loss: 0.3809\n",
            "Epoch 809/1000 | Loss: 0.3808\n",
            "Epoch 810/1000 | Loss: 0.3807\n",
            "Epoch 811/1000 | Loss: 0.3806\n",
            "Epoch 812/1000 | Loss: 0.3806\n",
            "Epoch 813/1000 | Loss: 0.3805\n",
            "Epoch 814/1000 | Loss: 0.3804\n",
            "Epoch 815/1000 | Loss: 0.3803\n",
            "Epoch 816/1000 | Loss: 0.3802\n",
            "Epoch 817/1000 | Loss: 0.3801\n",
            "Epoch 818/1000 | Loss: 0.3800\n",
            "Epoch 819/1000 | Loss: 0.3799\n",
            "Epoch 820/1000 | Loss: 0.3798\n",
            "Epoch 821/1000 | Loss: 0.3797\n",
            "Epoch 822/1000 | Loss: 0.3796\n",
            "Epoch 823/1000 | Loss: 0.3795\n",
            "Epoch 824/1000 | Loss: 0.3794\n",
            "Epoch 825/1000 | Loss: 0.3794\n",
            "Epoch 826/1000 | Loss: 0.3793\n",
            "Epoch 827/1000 | Loss: 0.3792\n",
            "Epoch 828/1000 | Loss: 0.3791\n",
            "Epoch 829/1000 | Loss: 0.3790\n",
            "Epoch 830/1000 | Loss: 0.3789\n",
            "Epoch 831/1000 | Loss: 0.3788\n",
            "Epoch 832/1000 | Loss: 0.3787\n",
            "Epoch 833/1000 | Loss: 0.3786\n",
            "Epoch 834/1000 | Loss: 0.3785\n",
            "Epoch 835/1000 | Loss: 0.3784\n",
            "Epoch 836/1000 | Loss: 0.3783\n",
            "Epoch 837/1000 | Loss: 0.3783\n",
            "Epoch 838/1000 | Loss: 0.3782\n",
            "Epoch 839/1000 | Loss: 0.3781\n",
            "Epoch 840/1000 | Loss: 0.3780\n",
            "Epoch 841/1000 | Loss: 0.3779\n",
            "Epoch 842/1000 | Loss: 0.3778\n",
            "Epoch 843/1000 | Loss: 0.3777\n",
            "Epoch 844/1000 | Loss: 0.3776\n",
            "Epoch 845/1000 | Loss: 0.3775\n",
            "Epoch 846/1000 | Loss: 0.3774\n",
            "Epoch 847/1000 | Loss: 0.3773\n",
            "Epoch 848/1000 | Loss: 0.3773\n",
            "Epoch 849/1000 | Loss: 0.3772\n",
            "Epoch 850/1000 | Loss: 0.3771\n",
            "Epoch 851/1000 | Loss: 0.3770\n",
            "Epoch 852/1000 | Loss: 0.3769\n",
            "Epoch 853/1000 | Loss: 0.3768\n",
            "Epoch 854/1000 | Loss: 0.3767\n",
            "Epoch 855/1000 | Loss: 0.3766\n",
            "Epoch 856/1000 | Loss: 0.3765\n",
            "Epoch 857/1000 | Loss: 0.3764\n",
            "Epoch 858/1000 | Loss: 0.3764\n",
            "Epoch 859/1000 | Loss: 0.3763\n",
            "Epoch 860/1000 | Loss: 0.3762\n",
            "Epoch 861/1000 | Loss: 0.3761\n",
            "Epoch 862/1000 | Loss: 0.3760\n",
            "Epoch 863/1000 | Loss: 0.3759\n",
            "Epoch 864/1000 | Loss: 0.3758\n",
            "Epoch 865/1000 | Loss: 0.3757\n",
            "Epoch 866/1000 | Loss: 0.3756\n",
            "Epoch 867/1000 | Loss: 0.3755\n",
            "Epoch 868/1000 | Loss: 0.3755\n",
            "Epoch 869/1000 | Loss: 0.3754\n",
            "Epoch 870/1000 | Loss: 0.3753\n",
            "Epoch 871/1000 | Loss: 0.3752\n",
            "Epoch 872/1000 | Loss: 0.3751\n",
            "Epoch 873/1000 | Loss: 0.3750\n",
            "Epoch 874/1000 | Loss: 0.3749\n",
            "Epoch 875/1000 | Loss: 0.3748\n",
            "Epoch 876/1000 | Loss: 0.3747\n",
            "Epoch 877/1000 | Loss: 0.3747\n",
            "Epoch 878/1000 | Loss: 0.3746\n",
            "Epoch 879/1000 | Loss: 0.3745\n",
            "Epoch 880/1000 | Loss: 0.3744\n",
            "Epoch 881/1000 | Loss: 0.3743\n",
            "Epoch 882/1000 | Loss: 0.3742\n",
            "Epoch 883/1000 | Loss: 0.3741\n",
            "Epoch 884/1000 | Loss: 0.3740\n",
            "Epoch 885/1000 | Loss: 0.3739\n",
            "Epoch 886/1000 | Loss: 0.3739\n",
            "Epoch 887/1000 | Loss: 0.3738\n",
            "Epoch 888/1000 | Loss: 0.3737\n",
            "Epoch 889/1000 | Loss: 0.3736\n",
            "Epoch 890/1000 | Loss: 0.3735\n",
            "Epoch 891/1000 | Loss: 0.3734\n",
            "Epoch 892/1000 | Loss: 0.3733\n",
            "Epoch 893/1000 | Loss: 0.3732\n",
            "Epoch 894/1000 | Loss: 0.3731\n",
            "Epoch 895/1000 | Loss: 0.3731\n",
            "Epoch 896/1000 | Loss: 0.3730\n",
            "Epoch 897/1000 | Loss: 0.3729\n",
            "Epoch 898/1000 | Loss: 0.3728\n",
            "Epoch 899/1000 | Loss: 0.3727\n",
            "Epoch 900/1000 | Loss: 0.3726\n",
            "Epoch 901/1000 | Loss: 0.3725\n",
            "Epoch 902/1000 | Loss: 0.3724\n",
            "Epoch 903/1000 | Loss: 0.3724\n",
            "Epoch 904/1000 | Loss: 0.3723\n",
            "Epoch 905/1000 | Loss: 0.3722\n",
            "Epoch 906/1000 | Loss: 0.3721\n",
            "Epoch 907/1000 | Loss: 0.3720\n",
            "Epoch 908/1000 | Loss: 0.3719\n",
            "Epoch 909/1000 | Loss: 0.3718\n",
            "Epoch 910/1000 | Loss: 0.3717\n",
            "Epoch 911/1000 | Loss: 0.3717\n",
            "Epoch 912/1000 | Loss: 0.3716\n",
            "Epoch 913/1000 | Loss: 0.3715\n",
            "Epoch 914/1000 | Loss: 0.3714\n",
            "Epoch 915/1000 | Loss: 0.3713\n",
            "Epoch 916/1000 | Loss: 0.3712\n",
            "Epoch 917/1000 | Loss: 0.3711\n",
            "Epoch 918/1000 | Loss: 0.3710\n",
            "Epoch 919/1000 | Loss: 0.3710\n",
            "Epoch 920/1000 | Loss: 0.3709\n",
            "Epoch 921/1000 | Loss: 0.3708\n",
            "Epoch 922/1000 | Loss: 0.3707\n",
            "Epoch 923/1000 | Loss: 0.3706\n",
            "Epoch 924/1000 | Loss: 0.3705\n",
            "Epoch 925/1000 | Loss: 0.3704\n",
            "Epoch 926/1000 | Loss: 0.3703\n",
            "Epoch 927/1000 | Loss: 0.3703\n",
            "Epoch 928/1000 | Loss: 0.3702\n",
            "Epoch 929/1000 | Loss: 0.3701\n",
            "Epoch 930/1000 | Loss: 0.3700\n",
            "Epoch 931/1000 | Loss: 0.3699\n",
            "Epoch 932/1000 | Loss: 0.3698\n",
            "Epoch 933/1000 | Loss: 0.3697\n",
            "Epoch 934/1000 | Loss: 0.3697\n",
            "Epoch 935/1000 | Loss: 0.3696\n",
            "Epoch 936/1000 | Loss: 0.3695\n",
            "Epoch 937/1000 | Loss: 0.3694\n",
            "Epoch 938/1000 | Loss: 0.3693\n",
            "Epoch 939/1000 | Loss: 0.3692\n",
            "Epoch 940/1000 | Loss: 0.3691\n",
            "Epoch 941/1000 | Loss: 0.3690\n",
            "Epoch 942/1000 | Loss: 0.3690\n",
            "Epoch 943/1000 | Loss: 0.3689\n",
            "Epoch 944/1000 | Loss: 0.3688\n",
            "Epoch 945/1000 | Loss: 0.3687\n",
            "Epoch 946/1000 | Loss: 0.3686\n",
            "Epoch 947/1000 | Loss: 0.3685\n",
            "Epoch 948/1000 | Loss: 0.3684\n",
            "Epoch 949/1000 | Loss: 0.3684\n",
            "Epoch 950/1000 | Loss: 0.3683\n",
            "Epoch 951/1000 | Loss: 0.3682\n",
            "Epoch 952/1000 | Loss: 0.3681\n",
            "Epoch 953/1000 | Loss: 0.3680\n",
            "Epoch 954/1000 | Loss: 0.3679\n",
            "Epoch 955/1000 | Loss: 0.3678\n",
            "Epoch 956/1000 | Loss: 0.3678\n",
            "Epoch 957/1000 | Loss: 0.3677\n",
            "Epoch 958/1000 | Loss: 0.3676\n",
            "Epoch 959/1000 | Loss: 0.3675\n",
            "Epoch 960/1000 | Loss: 0.3674\n",
            "Epoch 961/1000 | Loss: 0.3673\n",
            "Epoch 962/1000 | Loss: 0.3673\n",
            "Epoch 963/1000 | Loss: 0.3672\n",
            "Epoch 964/1000 | Loss: 0.3671\n",
            "Epoch 965/1000 | Loss: 0.3670\n",
            "Epoch 966/1000 | Loss: 0.3669\n",
            "Epoch 967/1000 | Loss: 0.3668\n",
            "Epoch 968/1000 | Loss: 0.3667\n",
            "Epoch 969/1000 | Loss: 0.3667\n",
            "Epoch 970/1000 | Loss: 0.3666\n",
            "Epoch 971/1000 | Loss: 0.3665\n",
            "Epoch 972/1000 | Loss: 0.3664\n",
            "Epoch 973/1000 | Loss: 0.3663\n",
            "Epoch 974/1000 | Loss: 0.3662\n",
            "Epoch 975/1000 | Loss: 0.3662\n",
            "Epoch 976/1000 | Loss: 0.3661\n",
            "Epoch 977/1000 | Loss: 0.3660\n",
            "Epoch 978/1000 | Loss: 0.3659\n",
            "Epoch 979/1000 | Loss: 0.3658\n",
            "Epoch 980/1000 | Loss: 0.3657\n",
            "Epoch 981/1000 | Loss: 0.3656\n",
            "Epoch 982/1000 | Loss: 0.3656\n",
            "Epoch 983/1000 | Loss: 0.3655\n",
            "Epoch 984/1000 | Loss: 0.3654\n",
            "Epoch 985/1000 | Loss: 0.3653\n",
            "Epoch 986/1000 | Loss: 0.3652\n",
            "Epoch 987/1000 | Loss: 0.3651\n",
            "Epoch 988/1000 | Loss: 0.3651\n",
            "Epoch 989/1000 | Loss: 0.3650\n",
            "Epoch 990/1000 | Loss: 0.3649\n",
            "Epoch 991/1000 | Loss: 0.3648\n",
            "Epoch 992/1000 | Loss: 0.3647\n",
            "Epoch 993/1000 | Loss: 0.3646\n",
            "Epoch 994/1000 | Loss: 0.3646\n",
            "Epoch 995/1000 | Loss: 0.3645\n",
            "Epoch 996/1000 | Loss: 0.3644\n",
            "Epoch 997/1000 | Loss: 0.3643\n",
            "Epoch 998/1000 | Loss: 0.3642\n",
            "Epoch 999/1000 | Loss: 0.3641\n",
            "Epoch 1000/1000 | Loss: 0.3641\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.2667 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9919 | Above 50%: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO9_wjBlY7H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}